# -*- coding: utf-8 -*-
"""
Speculative Decoding Scorer è®­ç»ƒè„šæœ¬
ä½¿ç”¨ verl æ¡†æ¶çš„ GRPO ç®—æ³•è®­ç»ƒ SPD Scoring Model

æ ¸å¿ƒæ€æƒ³:
    - å°† SPD Scorer çš„ Accept/Reject å†³ç­–å»ºæ¨¡ä¸ºä¸€ç§ç‰¹æ®Šçš„"åºåˆ—ç”Ÿæˆ"ä»»åŠ¡
    - è¾“å…¥: [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
    - è¾“å‡º: å¯¹æ¯ä¸ª Draft Token ä½ç½®çš„ Accept/Reject å†³ç­–
    - Reward: åŸºäºå››åœºæ™¯é€»è¾‘è®¡ç®— (åŠ é€ŸæˆåŠŸ/ç ´åæ­£ç¡®/æ— ç”¨å°è¯•/çº æ­£é”™è¯¯)

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-11-25
"""

import os
import sys
import subprocess
from typing import Optional, Dict, List, Tuple

import torch
import pandas as pd
from loguru import logger

from transformers import AutoTokenizer

# ç¡®ä¿å½“å‰ç›®å½•åœ¨ sys.path ä¸­
sys.path.insert(0, os.getcwd())

# ==============================================================================
# Monkey Patch: æ³¨å†Œ SPD Scorer æ¨¡å‹
# ==============================================================================
import verl.utils.model
from spd_scorer import AutoModelForSPDScoring

# å¼ºåˆ¶æ³¨å…¥è‡ªå®šä¹‰çš„æ¨¡å‹åŠ è½½é€»è¾‘
_original_create_huggingface_actor = verl.utils.model.create_huggingface_actor

def _patched_create_huggingface_actor(model_name: str, override_config_kwargs=None, automodel_kwargs=None) -> torch.nn.Module:
    """
    Hook åçš„ create_huggingface_actor å‡½æ•°ï¼Œæ‹¦æˆªå¹¶åŠ è½½ SPD Scorer
    """
    if override_config_kwargs is None:
        override_config_kwargs = {}
    if automodel_kwargs is None:
        automodel_kwargs = {}
        
    logger.info(f"[Patch] Intercepting model loading for: {model_name}")
    logger.info(f"[Patch] Loading AutoModelForSPDScoring...")
        
    # è·å– HF Config
    module_config = verl.utils.model.get_huggingface_actor_config(
        model_name, override_config_kwargs, trust_remote_code=automodel_kwargs.get("trust_remote_code", False)
    )
    
    # ä½¿ç”¨ SPD Scorer Factory åŠ è½½æ¨¡å‹
    # æ³¨æ„: è¿™é‡Œä¼šè°ƒç”¨ ScoringActor çš„åˆå§‹åŒ–ï¼Œå†…éƒ¨å¯èƒ½ä¼šå†æ¬¡åŠ è½½ Backbone
    # ä½†ç”±äº vLLM å’Œ HF çš„ç¼“å­˜æœºåˆ¶ï¼Œæˆ–è€…å•çº¯çš„å¤šæ¬¡åŠ è½½ï¼Œåªè¦æ˜¾å­˜è¶³å¤Ÿï¼Œæ˜¯å¯ä»¥æ¥å—çš„
    model = AutoModelForSPDScoring.from_config(module_config, **automodel_kwargs)
    
    return model

# åº”ç”¨ Patch: æ›¿æ¢ verl.utils.model ä¸­çš„å‡½æ•°
verl.utils.model.create_huggingface_actor = _patched_create_huggingface_actor
logger.info("âœ… å·²åº”ç”¨ Monkey Patch: verl.utils.model.create_huggingface_actor -> AutoModelForSPDScoring")

# ==============================================================================
# 2. æ•°æ®å‡†å¤‡
# ==============================================================================
def prepare_spd_data_from_real_source(
    args,
    source_data_path: List[str],
) -> Tuple[str, str]:
    """
    ä»çœŸå®æ•°æ®æº (SPDç”Ÿæˆæ•°æ® + Metadata) å‡†å¤‡ SPD è®­ç»ƒæ•°æ®
    
    Args:
        args: è®­ç»ƒé…ç½®å‚æ•° (argparse.Namespace)
        source_data_path: åŒ…å«ä¸¤ä¸ªæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ [spd_gen_data_file, metadata_file]
    """
    # 1. è®¡ç®—ç›®æ ‡è·¯å¾„
    train_file = args.train_file
    val_file = args.val_file
    
    train_path = os.path.join(args.data_dir, train_file)
    val_path = os.path.join(args.data_dir, val_file)
    
    # 2. æ£€æŸ¥æ˜¯å¦è·³è¿‡ (Cache Hit)
    overwrite = getattr(args, "overwrite_data", False)
    if os.path.exists(train_path) and os.path.exists(val_path) and not overwrite:
        logger.info(f"è®­ç»ƒæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡é¢„å¤„ç†æ­¥éª¤ã€‚")
        logger.info(f"Train: {train_path}")
        logger.info(f"Val:   {val_path}")
        return train_path, val_path

    if len(source_data_path) != 2:
        raise ValueError("source_data_path å¿…é¡»æ˜¯åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„åˆ—è¡¨: [spd_gen_data_file, metadata_file]")
    
    spd_gen_data_file = source_data_path[0]
    meta_file = source_data_path[1]
    logger.info(f"SPD Gen Data File: {spd_gen_data_file}")
    logger.info(f"Metadata File: {meta_file}")
    
    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
    logger.info(f"Tokenizer åŠ è½½æˆåŠŸ: {args.model_path}")
    
    # 1. åŠ è½½ Metadata åˆ°å†…å­˜å­—å…¸ (Index -> Data)
    logger.info("åŠ è½½ Metadata...")
    meta_df = pd.read_json(meta_file, lines=True)
    # index å­—æ®µå³ä¸º sample_idx
    meta_dict = meta_df.set_index("index").to_dict(orient="index")
    logger.info(f"Metadata æ•°æ®é‡: {len(meta_dict)}")
    
    # 2. éå† SPD ç”Ÿæˆæ•°æ®å¹¶å¤„ç†
    logger.info("å¤„ç† SPD ç”Ÿæˆæ•°æ®...")
    spd_gen_df = pd.read_json(spd_gen_data_file, lines=True)
    logger.info(f"SPD ç”Ÿæˆæ•°æ®é‡: {len(spd_gen_df)}")
    
    os.makedirs(args.data_dir, exist_ok=True)
    processed_data = []
    
    for idx, row in spd_gen_df.iterrows():
        if idx % 1000 == 0:
            logger.info(f"å¤„ç†è¿›åº¦: {idx}/{len(spd_gen_df)}")
            
        sample_idx = row.get('sample_idx')
        cut_idx = row.get('cut_idx')
        
        # æŸ¥æ‰¾ Metadata
        if sample_idx not in meta_dict:
            logger.warning(f"Sample {sample_idx} not found in metadata. Skipping.")
            continue
            
        meta = meta_dict[sample_idx]
        
        problem = meta.get('problem')
        ground_truth = meta.get('reference_answer')
        is_correct_baseline = meta.get('is_correct')
        
        # æ„é€  Context IDs
        # Step A: Base Chat (System + User)
        base_messages = [
            {
                "role": "system",
                "content": "Please reason step by step, and put your final answer within \\boxed{}.",
            },
            {
                "role": "user",
                "content": problem,
            },
        ]
        
        # apply_chat_template è¿”å› tensor if return_tensors='pt'
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦ list[int] ä»¥ä¾¿æ‹¼æ¥
        base_ids = tokenizer.apply_chat_template(
            base_messages,
            tokenize=True,
            add_generation_prompt=True
        )
        
        # Step B: Answer Prefix
        full_answer_ids = meta.get('answer_ids')
        answer_prefix_ids = full_answer_ids[:cut_idx]
        
        # Step C: Splice Context
        context_ids = base_ids + answer_prefix_ids
        
        # è·å– Draft / Target IDs
        draft_ids = row.get('draft_output_ids')
        target_ids = row.get('target_output_ids')
        
        # ä¸¥æ ¼æ ¡éªŒé•¿åº¦: Target åº”è¯¥æ¯” Draft å¤šä¸€ä¸ª bonus token
        if len(target_ids) != len(draft_ids) + 1:
            logger.warning(f"Sample {sample_idx} draft/target length mismatch: target ({len(target_ids)}) should be draft ({len(draft_ids)}) + 1. Skipping.")
            continue
        if len(draft_ids) == 0:
            logger.warning(f"Sample {sample_idx} draft length is 0. Skipping.")
            continue
            
        draft_len = len(draft_ids)
        target_len = len(target_ids) - 1  # å»æ‰ bonus token åçš„é•¿åº¦ï¼Œåº”è¯¥ç­‰äº draft_len

        # =================================================================
        # æ„é€ å®Œæ•´çš„ Input IDs (ç”¨äº Actor è¾“å…¥)
        # ç»“æ„: [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
        # =================================================================
        
        # è·å– SEP Token ID (Llama-3 eot_id)
        if args.sep_token_id == "eot":
            sep_token_id = tokenizer.eos_token_id if hasattr(tokenizer, "eos_token_id") else tokenizer.eot_token_id
        else:
            raise ValueError(f"Invalid sep_token_id: {args.sep_token_id}")
        
        # æ‹¼æ¥
        # æ³¨æ„: è¿™é‡Œå‡è®¾ draft_ids å’Œ target_ids å·²ç»æ˜¯ list[int]
        full_input_ids = (
            context_ids + 
            [sep_token_id] + 
            draft_ids + 
            [sep_token_id] + 
            target_ids[:-1] + 
            [sep_token_id]
        )
        
        # è®¡ç®—å…³é”®ä½ç½®ç´¢å¼• (ç”¨äºåç»­ Mask ç”Ÿæˆå’Œé€»è¾‘å¤„ç†)
        # draft_start_idx: Draft Tokens çš„èµ·å§‹ä½ç½® (åŒ…å«å‰é¢çš„ SEP)
        # å®é™…ä¸Šåœ¨ list ç´¢å¼•ä¸­ï¼Œdraft_start_idx æŒ‡å‘çš„æ˜¯ Draft çš„ç¬¬ä¸€ä¸ª Token
        # context_len (åŒ…å« SEP) = len(context_ids) + 1
        draft_start_idx = len(context_ids) + 1 
        
        # draft_end_idx: Draft Tokens çš„ç»“æŸä½ç½® (ä¸åŒ…å«åé¢çš„ SEP)
        draft_end_idx = draft_start_idx + draft_len
        
        # target_start_idx: Target Tokens çš„èµ·å§‹ä½ç½®
        # å‰é¢æœ‰: Context + SEP + Draft + SEP
        target_start_idx = draft_end_idx + 1
        
        # target_end_idx: Target Tokens çš„ç»“æŸä½ç½®
        target_end_idx = target_start_idx + target_len
            
        # æ„é€ æ ·æœ¬
        sample = {
            "data_source": "spd_scorer",
            
            # (1) ä¸ºäº†å…¼å®¹ verl Dataset æ¥å£ï¼Œè¿™é‡Œæ”¾ä¸€ä¸ª dummy prompt (å®é™…ä¸Šæˆ‘ä»¬çš„ rollout/model åº”è¯¥ç›´æ¥è¯»å– input_ids)
            "prompt": "dummy_prompt", 
            
            # =====================================================
            # æ ¸å¿ƒæ•°æ® (é¡¶å±‚å­—æ®µï¼Œæ–¹ä¾¿åç»­ Rollout/Training ç›´æ¥è¯»å–)
            # =====================================================
            "input_ids": full_input_ids,    # [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
            
            # =====================================================
            # verl å…¼å®¹å­—æ®µ
            # =====================================================
            "ability": "spd_scoring",
            "reward_model": {
                "style": "rule",
                # (2) å°† Ground Truth æ”¾å…¥ reward_model å­—æ®µï¼Œè¿™æ˜¯ verl çš„æƒ¯ä¾‹
                "ground_truth": ground_truth,
            },
            "extra_info": {
                "split": "train",
                "index": idx,
                "context_ids": context_ids,
                "draft_tokens": draft_ids,
                "target_tokens": target_ids[:-1],
                "bonus_tokens": target_ids[-1:],
                "is_correct_baseline": is_correct_baseline,
                "draft_len": draft_len,
                # ä½ç½®ä¿¡æ¯ (Non-Tensor, ä½†å¯ä»¥åœ¨ Rollout ä¸­è½¬ä¸º Tensor)
                "draft_start_idx": draft_start_idx,
                "draft_end_idx": draft_end_idx,
                "target_start_idx": target_start_idx,
                "target_end_idx": target_end_idx,
            }
        }
        processed_data.append(sample)
    
    # ä¿å­˜
    df = pd.DataFrame(processed_data)
    train_size = int(len(df) * 0.95)
    train_df = df.iloc[:train_size]
    val_df = df.iloc[train_size:]
    
    train_df.to_parquet(train_path)
    val_df.to_parquet(val_path)
    
    logger.info(f"å¤„ç†å®Œæˆ: è®­ç»ƒé›† {len(train_df)} æ¡, éªŒè¯é›† {len(val_df)} æ¡")
    
    return train_path, val_path


# ==============================================================================
# 3. è®­ç»ƒä¸»å‡½æ•°
# ==============================================================================

def build_training_command(args, train_file: str, val_file: str) -> list:
    """
    æ„å»º verl GRPO è®­ç»ƒå‘½ä»¤
    
    Args:
        args: è®­ç»ƒé…ç½®å‚æ•° (argparse.Namespace)
        train_file: è®­ç»ƒæ•°æ®è·¯å¾„
        val_file: éªŒè¯æ•°æ®è·¯å¾„
    
    Returns:
        cmd: è®­ç»ƒå‘½ä»¤åˆ—è¡¨
    """
    
    # åŸºç¡€å‘½ä»¤
    cmd = [
        sys.executable, "-m", "verl.trainer.main_ppo",
        
        # =================================================================
        # ç®—æ³•æ ¸å¿ƒé…ç½® (GRPO)
        # =================================================================
        "algorithm.adv_estimator=grpo",        # ä½¿ç”¨ GRPO ç®—æ³•
        "algorithm.use_kl_in_reward=False",    # GRPO ç‰¹æ€§
        "algorithm.kl_ctrl.kl_coef=0.001",     # KL æ•£åº¦ç³»æ•°
        
        # =================================================================
        # æ•°æ®é…ç½®
        # =================================================================
        f"data.train_files={train_file}",
        f"data.val_files={val_file}",
        
        # ä½¿ç”¨è‡ªå®šä¹‰çš„ SPD Datasetï¼ˆé€šè¿‡ pkg:// æŒ‰æ¨¡å—å¯¼å…¥ï¼‰
        "data.custom_cls.path=pkg://verl.utils.dataset.spd_dataset",
        "data.custom_cls.name=SPDRLHFDataset",
        
        f"data.train_batch_size={args.train_batch_size}",
        "data.max_prompt_length=2048",         # åŒ…å« Context + Draft + Target
        "data.max_response_length=50",        # è¾“å‡ºæ˜¯ Accept/Reject å†³ç­–åºåˆ—
        
        # =================================================================
        # æ¨¡å‹é…ç½®
        # =================================================================
        f"actor_rollout_ref.model.path={args.model_path}",
        "actor_rollout_ref.model.use_remove_padding=True",
        
        # LoRA é…ç½®
        f"actor_rollout_ref.model.lora_rank={args.lora_rank}",
        f"actor_rollout_ref.model.lora_alpha={args.lora_alpha}",
        
        # =================================================================
        # Rollout é…ç½®
        # =================================================================
        f"actor_rollout_ref.rollout.n={args.rollout_n}",
        "actor_rollout_ref.rollout.name=spd",  # ä½¿ç”¨è‡ªå®šä¹‰çš„ SPD Rollout
        f"actor_rollout_ref.rollout.gpu_memory_utilization={args.vllm_gpu_memory_utilization}",
        "actor_rollout_ref.rollout.free_cache_engine=False",
        f"actor_rollout_ref.rollout.data_parallel_size={args.n_gpus}",
        "actor_rollout_ref.rollout.enforce_eager=True",
        "actor_rollout_ref.rollout.tensor_model_parallel_size=1",
        "actor_rollout_ref.rollout.enable_chunked_prefill=False",
        "actor_rollout_ref.rollout.max_num_batched_tokens=8192",
        f"actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu={args.micro_batch_size_per_gpu}",
        
        # =================================================================
        # Actor è®­ç»ƒé…ç½®
        # =================================================================
        f"actor_rollout_ref.actor.ppo_mini_batch_size={args.ppo_mini_batch_size}",
        f"actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu={args.micro_batch_size_per_gpu}",
        "actor_rollout_ref.actor.use_kl_loss=True",
        "actor_rollout_ref.actor.kl_loss_coef=0.001",
        f"actor_rollout_ref.actor.fsdp_config.param_offload={args.offload}",
        f"actor_rollout_ref.actor.fsdp_config.optimizer_offload={args.offload}",
        
        # =================================================================
        # Reference é…ç½®
        # =================================================================
        f"actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu={args.micro_batch_size_per_gpu}",
        f"actor_rollout_ref.ref.fsdp_config.param_offload={args.offload}",
        
        # =================================================================
        # Trainer é…ç½®
        # =================================================================
        f"trainer.total_epochs={args.total_epochs}",
        f"trainer.n_gpus_per_node={args.n_gpus}",
        "trainer.nnodes=1",
        f"trainer.project_name={args.project_name}",
        f"trainer.experiment_name={args.experiment_name}",
        "trainer.test_freq=10",
        "trainer.save_freq=-1",
    ]
    
    # æ—¥å¿—é…ç½®
    if not args.no_wandb:
        cmd.append("trainer.logger=['console','wandb']")
    else:
        cmd.append("trainer.logger=['console']")
    
    return cmd


def run_training(args, source_data_path: List[str]):
    """
    è¿è¡Œ SPD Scorer è®­ç»ƒ
    
    å®Œæ•´æµç¨‹:
        1. å‡†å¤‡è®­ç»ƒæ•°æ®
        2. æ„å»ºå¹¶æ‰§è¡Œè®­ç»ƒå‘½ä»¤
    
    Args:
        args: è®­ç»ƒé…ç½®å‚æ•° (argparse.Namespace)
        source_data_path: åŒ…å«ä¸¤ä¸ªæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ [spd_gen_data_file, metadata_file]
    """
    logger.info("=" * 60)
    logger.info("SPD Scorer GRPO è®­ç»ƒ")
    logger.info("=" * 60)
    
    # Step 1: å‡†å¤‡æ•°æ®
    logger.info("\n[Step 1] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    train_file, val_file = prepare_spd_data_from_real_source(args, source_data_path)
    
    # Step 2: æ„å»ºè®­ç»ƒå‘½ä»¤
    logger.info("\n[Step 2] æ„å»ºè®­ç»ƒå‘½ä»¤...")
    cmd = build_training_command(args, train_file, val_file)
    
    logger.info("\nè®­ç»ƒå‘½ä»¤:")
    logger.info(" ".join(cmd[:5]) + " \\")
    for arg in cmd[5:]:
        logger.info(f"    {arg} \\")
    
    # Step 3: æ‰§è¡Œè®­ç»ƒ
    logger.info("\n[Step 3] å¯åŠ¨è®­ç»ƒ...")
    logger.info("=" * 60)
    
    logger.info(f"ğŸš€ å¼€å§‹ SPD Scorer GRPO è®­ç»ƒ")
    logger.info(f"é…ç½®: {args.n_gpus} GPU | Batch={args.train_batch_size} | Rollout N={args.rollout_n} | Offload={'ON' if args.offload else 'OFF'}")
    logger.info("=" * 60)
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒå˜é‡
    env = _create_training_env(args)
    
    subprocess.run(cmd, check=True, env=env)
        
 

def _create_training_env(args) -> Dict[str, str]:
    """
    ç”Ÿæˆè®­ç»ƒå­è¿›ç¨‹æ‰€éœ€çš„ç¯å¢ƒå˜é‡
    
    åŒ…å«:
    1. Reward Function é…ç½®
    2. Model è·¯å¾„é…ç½® (ç”¨äº spd_scorer å’Œ spd_scorer_reward)
    3. åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
    """
    env = os.environ.copy()
    
    # 1. Reward é…ç½®
    env["SPD_REWARD_ALPHA"] = str(args.reward_alpha)
    env["SPD_REWARD_PENALTY_BREAK"] = str(args.reward_penalty_break)
    env["SPD_REWARD_CORRECT"] = str(args.reward_correct)
    env["SPD_REWARD_USELESS"] = str(args.reward_useless)
    
    # 2. æ¨¡å‹é…ç½® (æ³¨å…¥ç¯å¢ƒå˜é‡ï¼Œä¾› spd_scorer.py å’Œ spd_scorer_reward.py è¯»å–)
    env["SPD_MODEL_PATH"] = str(args.model_path)
    
    # Target Model Path: ä¼˜å…ˆä½¿ç”¨ target_model_pathï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œfallback åˆ° model_path
    env["SPD_TARGET_MODEL_PATH"] = str(args.target_model_path)
   
        
    env["SPD_LORA_RANK"] = str(args.lora_rank)
    env["SPD_LORA_ALPHA"] = str(args.lora_alpha)
    
    # å¤„ç† SEP Token ID
    # å¦‚æœæ˜¯ "eot"ï¼Œåˆ™å°è¯•åŠ è½½ tokenizer è§£æï¼Œæˆ–è€…ä½¿ç”¨é»˜è®¤å€¼
    if str(args.sep_token_id).lower() == "eot":
        logger.info(f"è§£æ sep_token_id='eot'...")
        tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True)
        # ä¼˜å…ˆå°è¯• eot_id (Llama-3), ç„¶å eos_token_id
        if hasattr(tokenizer, "eot_token_id") and tokenizer.eot_token_id is not None:
            real_sep_id = tokenizer.eot_token_id
        
        else:
            raise ValueError(f"æ— æ³•è§£æ sep_token_id: {args.sep_token_id}")
        logger.info(f"å·²è§£æ sep_token_id: eot -> {real_sep_id}")
        env["SPD_SEP_TOKEN_ID"] = str(real_sep_id)
        
    else:
        raise ValueError(f"Invalid sep_token_id: {args.sep_token_id}")
    
    # 3. åŸºç¡€è®­ç»ƒé…ç½®
    env["HYDRA_FULL_ERROR"] = "1"
    env["NCCL_P2P_DISABLE"] = "1"
    
    return env


# ==============================================================================
# 5. å‘½ä»¤è¡Œå…¥å£
# ==============================================================================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SPD Scorer GRPO è®­ç»ƒ")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_path", type=str, default="meta-llama/Llama-3-8B", help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--target_model_path", type=str, default=None, help="Target æ¨¡å‹è·¯å¾„ (ç”¨äº Reward Tokenizer)")
    parser.add_argument("--lora_rank", type=int, default=16, help="LoRA Rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA Alpha")
    
    # æ•°æ®é…ç½®
    parser.add_argument("--data_dir", type=str, default="data/spd_scorer", help="æ•°æ®ç›®å½•")
    parser.add_argument("--train_file", type=str, default="train.parquet", help="è®­ç»ƒæ•°æ®æ–‡ä»¶å")
    parser.add_argument("--val_file", type=str, default="val.parquet", help="éªŒè¯æ•°æ®æ–‡ä»¶å")
    parser.add_argument("--spd_gen_data_file", type=str, required=True, help="SPD ç”Ÿæˆæ•°æ®æ–‡ä»¶è·¯å¾„ (jsonl)")
    parser.add_argument("--metadata_file", type=str, required=True, help="Metadata æ–‡ä»¶è·¯å¾„ (jsonl)")
    parser.add_argument("--overwrite_data", action="store_true", help="æ˜¯å¦å¼ºåˆ¶è¦†ç›–å·²å­˜åœ¨çš„è®­ç»ƒæ•°æ®")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--n_gpus", type=int, default=8, help="GPU æ•°é‡")
    parser.add_argument("--train_batch_size", type=int, default=64, help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--rollout_n", type=int, default=8, help="GRPO Rollout N")
    parser.add_argument("--total_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--ppo_mini_batch_size", type=int, default=32, help="PPO mini batch å¤§å°")
    parser.add_argument("--micro_batch_size_per_gpu", type=int, default=4, help="æ¯ GPU å¾®æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--offload", action="store_true", help="å¯ç”¨ FSDP CPU Offload (çœæ˜¾å­˜ä½†é™ä½é€Ÿåº¦)")
    parser.add_argument("--vllm_gpu_memory_utilization", type=float, default=0.7, help="vLLM GPU æ˜¾å­˜åˆ©ç”¨ç‡")
    parser.add_argument("--sep_token_id", type=str, default="eot", help="åˆ†éš”ç¬¦ Token ID (eot æˆ–æ•°å­—)")
    
    # å¥–åŠ±é…ç½®
    parser.add_argument("--reward_alpha", type=float, default=1.0, help="åœºæ™¯Aå¥–åŠ±ç³»æ•°")
    parser.add_argument("--reward_penalty_break", type=float, default=-10.0, help="åœºæ™¯Bæƒ©ç½š")
    parser.add_argument("--reward_correct", type=float, default=100.0, help="åœºæ™¯Då¥–åŠ±")
    parser.add_argument("--reward_useless", type=float, default=0.0, help="åœºæ™¯Cå¥–åŠ±")
    
    # å…¶ä»–
    parser.add_argument("--no_wandb", action="store_true", help="ç¦ç”¨ WandB")
    parser.add_argument("--project_name", type=str, default="verl_spd_scorer", help="WandB é¡¹ç›®å")
    parser.add_argument("--experiment_name", type=str, default="spd_grpo_training", help="å®éªŒå")
    
    args = parser.parse_args()
    
    # æ‰“å°é…ç½®
    logger.info("\n" + "=" * 60)
    logger.info("SPD Scorer è®­ç»ƒé…ç½®")
    logger.info("=" * 60)
    for key, value in vars(args).items():
        logger.info(f"{key}: {value}")
    logger.info("=" * 60 + "\n")
    
    source_data_path = [args.spd_gen_data_file, args.metadata_file]
    run_training(args, source_data_path)
    


if __name__ == "__main__":
    main()

