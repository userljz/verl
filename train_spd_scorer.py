# -*- coding: utf-8 -*-
"""
Speculative Decoding Scorer è®­ç»ƒè„šæœ¬
ä½¿ç”¨ verl æ¡†æ¶çš„ GRPO ç®—æ³•è®­ç»ƒ SPD Scoring Model

æ ¸å¿ƒæ€æƒ³:
    - å°† SPD Scorer çš„ Accept/Reject å†³ç­–å»ºæ¨¡ä¸ºä¸€ç§ç‰¹æ®Šçš„"åºåˆ—ç”Ÿæˆ"ä»»åŠ¡
    - è¾“å…¥: [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
    - è¾“å‡º: å¯¹æ¯ä¸ª Draft Token ä½ç½®çš„ Accept/Reject å†³ç­–
    - Reward: åŸºäºå››åœºæ™¯é€»è¾‘è®¡ç®— (åŠ é€ŸæˆåŠŸ/ç ´åæ­£ç¡®/æ— ç”¨å°è¯•/çº æ­£é”™è¯¯)

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-11-25
"""

import os
import sys
import json
import subprocess
import logging
from typing import Optional, Dict, List, Any, Tuple
from dataclasses import dataclass, field
import random

import torch
import pandas as pd

# ç¡®ä¿å½“å‰ç›®å½•åœ¨ sys.path ä¸­
sys.path.insert(0, os.getcwd())

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ==============================================================================
# 1. é…ç½®å®šä¹‰
# ==============================================================================

@dataclass
class SPDTrainingConfig:
    """SPD Scorer è®­ç»ƒé…ç½®"""
    
    # æ¨¡å‹è·¯å¾„
    model_path: str = "meta-llama/Llama-3-8B"
    
    # æ•°æ®è·¯å¾„
    data_dir: str = "data/spd_scorer"
    train_file: str = "train.parquet"
    val_file: str = "val.parquet"
    
    # LoRA é…ç½®
    lora_rank: int = 16
    lora_alpha: int = 32
    lora_target_modules: List[str] = field(default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj"])
    
    # è®­ç»ƒè¶…å‚æ•°
    n_gpus: int = 8
    train_batch_size: int = 64
    ppo_mini_batch_size: int = 32
    micro_batch_size_per_gpu: int = 4
    rollout_n: int = 8  # GRPO: æ¯ä¸ªæ ·æœ¬é‡‡æ · N ä¸ªå†³ç­–åºåˆ—
    total_epochs: int = 3
    
    # å¥–åŠ±ç³»æ•°
    reward_alpha: float = 1.0          # åœºæ™¯ A: alpha * L
    reward_penalty_break: float = -10.0  # åœºæ™¯ B: ç ´åæ­£ç¡®ç­”æ¡ˆ
    reward_correct: float = 100.0       # åœºæ™¯ D: çº æ­£é”™è¯¯
    reward_useless: float = 0.0         # åœºæ™¯ C: æ— ç”¨å°è¯•
    
    # ç‰¹æ®Š Token
    sep_token: str = "<|sep|>"
    sep_token_id: int = 128009  # Llama-3 çš„ <|eot_id|>ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    
    # è¡¥å…¨æœåŠ¡é…ç½®
    target_model_url: Optional[str] = None  # e.g. "http://localhost:8000/v1/completions"
    target_model_name: str = "target-model"
    
    # å…¶ä»–é…ç½®
    vllm_gpu_memory_utilization: float = 0.7
    offload: bool = False
    use_wandb: bool = True
    project_name: str = "verl_spd_scorer"
    experiment_name: str = "spd_grpo_training"


# ==============================================================================
# 2. æ•°æ®å‡†å¤‡
# ==============================================================================

def prepare_spd_training_data(
    config: SPDTrainingConfig,
    num_samples: int = 10000,
    max_context_len: int = 512,
    max_draft_len: int = 32,
    seed: int = 42
) -> Tuple[str, str]:
    """
    å‡†å¤‡ SPD Scorer çš„è®­ç»ƒæ•°æ®
    
    æ•°æ®æ ¼å¼è¯´æ˜:
        - æ¯ä¸ªæ ·æœ¬åŒ…å«: Context, Draft Tokens, Target Tokens
        - è®­ç»ƒç›®æ ‡: å­¦ä¹ å“ªäº› Mismatch çš„ Draft Token åº”è¯¥è¢«æ¥å—
    
    å®é™…ä½¿ç”¨æ—¶ï¼Œä½ åº”è¯¥ä»çœŸå®çš„ Speculative Decoding åœºæ™¯ä¸­æ”¶é›†æ•°æ®:
        1. ä½¿ç”¨ Draft Model ç”Ÿæˆ draft tokens
        2. ä½¿ç”¨ Target Model éªŒè¯å¹¶ç”Ÿæˆ target tokens
        3. è®°å½•æœ€ç»ˆç­”æ¡ˆæ˜¯å¦æ­£ç¡®
    
    Args:
        config: è®­ç»ƒé…ç½®
        num_samples: ç”Ÿæˆçš„æ ·æœ¬æ•°é‡
        max_context_len: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        max_draft_len: æœ€å¤§ Draft é•¿åº¦
        seed: éšæœºç§å­
    
    Returns:
        train_path, val_path: è®­ç»ƒå’ŒéªŒè¯æ•°æ®è·¯å¾„
    """
    logger.info(f"å‡†å¤‡ SPD è®­ç»ƒæ•°æ®ï¼Œç›®æ ‡ç›®å½•: {config.data_dir}")
    os.makedirs(config.data_dir, exist_ok=True)
    
    random.seed(seed)
    torch.manual_seed(seed)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    # æ³¨æ„: å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™é‡Œåº”è¯¥åŠ è½½çœŸå®çš„ Speculative Decoding æ•°æ®
    processed_data = []
    
    for i in range(num_samples):
        if i % 1000 == 0:
            logger.info(f"å·²ç”Ÿæˆ {i}/{num_samples} æ¡æ•°æ®...")
        
        # æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆ
        # å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™äº›åº”è¯¥æ¥è‡ªçœŸå®çš„ Draft/Target Model è¾“å‡º
        draft_len = random.randint(8, max_draft_len)
        
        # æ¨¡æ‹Ÿ Context (è¿™é‡Œç”¨å ä½ç¬¦ï¼Œå®é™…åº”è¯¥æ˜¯çœŸå®æ–‡æœ¬)
        context_text = f"Context for sample {i}. " * random.randint(1, 5)
        
        # æ¨¡æ‹Ÿ Draft å’Œ Target tokens (ç”¨ token ID è¡¨ç¤º)
        # å®é™…ä½¿ç”¨æ—¶ï¼Œè¿™äº›åº”è¯¥æ˜¯çœŸå®çš„ token IDs
        draft_tokens = [random.randint(1000, 30000) for _ in range(draft_len)]
        
        # Target tokens: ä¸ Draft æœ‰ä¸€å®šé‡åˆ (çº¦ 70% ç›¸åŒ)
        target_tokens = []
        for dt in draft_tokens:
            if random.random() < 0.7:
                target_tokens.append(dt)  # Match
            else:
                target_tokens.append(random.randint(1000, 30000))  # Mismatch
        
        # æ¨¡æ‹Ÿ Baseline æ­£ç¡®æ€§ (Target Model å•ç‹¬æ˜¯å¦ç­”å¯¹)
        is_correct_baseline = random.random() < 0.6  # çº¦ 60% æ­£ç¡®ç‡
        
        # æ¨¡æ‹Ÿ Ground Truth (æœ€ç»ˆæ­£ç¡®ç­”æ¡ˆ)
        ground_truth = f"answer_{i % 100}"
        
        # æ„é€  verl åè®®æ ¼å¼çš„æ•°æ®
        # å…³é”®: prompt æ„é€ ä¸º [Context + SEP + Draft + SEP + Target + SEP]
        sample = {
            # data_source ç”¨äºæŒ‡å®š reward function
            # æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰çš„ "spd_scorer" data_source
            "data_source": "spd_scorer",
            
            # Prompt: ä½¿ç”¨ Chat æ ¼å¼
            # å®é™…è¾“å…¥ä¼šåœ¨ tokenize åå˜æˆ [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
            "prompt": [
                {
                    "role": "system", 
                    "content": "You are a scoring model for speculative decoding. "
                               "Decide which draft tokens to accept."
                },
                {
                    "role": "user",
                    "content": json.dumps({
                        "context": context_text,
                        "draft_tokens": draft_tokens,
                        "target_tokens": target_tokens,
                    })
                }
            ],
            
            "ability": "spd_scoring",
            
            # reward_model å­—æ®µ: åŒ…å«è®¡ç®— reward æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
            "reward_model": {
                "style": "rule",
                "ground_truth": ground_truth,
                "draft_tokens": draft_tokens,
                "target_tokens": target_tokens,
                "is_correct_baseline": is_correct_baseline,
                "draft_len": draft_len,
                # å¥–åŠ±å‚æ•°
                "alpha": config.reward_alpha,
                "penalty_break": config.reward_penalty_break,
                "reward_correct": config.reward_correct,
                "reward_useless": config.reward_useless,
            },
            
            # é¢å¤–ä¿¡æ¯
            "extra_info": {
                "split": "train",
                "index": i,
                "draft_len": draft_len,
                "match_ratio": sum(1 for d, t in zip(draft_tokens, target_tokens) if d == t) / draft_len,
                # ä¼ é€’ç»™ Reward Function çš„å…³é”®ä¿¡æ¯
                "draft_tokens": draft_tokens,
                "target_tokens": target_tokens,
                "is_correct_baseline": is_correct_baseline,
                "alpha": config.reward_alpha,
                "penalty_break": config.reward_penalty_break,
                "reward_correct": config.reward_correct,
                "reward_useless": config.reward_useless,
                # ä¸Šä¸‹æ–‡å’Œè¡¥å…¨æœåŠ¡é…ç½®
                "context_text": context_text,
                "target_model_url": config.target_model_url,
                "target_model_name": config.target_model_name,
                "model_path": config.model_path,
            }
        }
        
        processed_data.append(sample)
    
    # è½¬æ¢ä¸º DataFrame
    df = pd.DataFrame(processed_data)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (95% è®­ç»ƒ, 5% éªŒè¯)
    train_size = int(len(df) * 0.95)
    train_df = df.iloc[:train_size]
    val_df = df.iloc[train_size:]
    
    # ä¿å­˜ä¸º Parquet æ–‡ä»¶
    train_path = os.path.join(config.data_dir, config.train_file)
    val_path = os.path.join(config.data_dir, config.val_file)
    
    train_df.to_parquet(train_path)
    val_df.to_parquet(val_path)
    
    logger.info(f"æ•°æ®å·²ä¿å­˜: è®­ç»ƒé›† {len(train_df)} æ¡, éªŒè¯é›† {len(val_df)} æ¡")
    logger.info(f"  - è®­ç»ƒé›†è·¯å¾„: {train_path}")
    logger.info(f"  - éªŒè¯é›†è·¯å¾„: {val_path}")
    
    return train_path, val_path


def prepare_spd_data_from_real_source(
    config: SPDTrainingConfig,
    source_data_path: str,
    tokenizer_path: Optional[str] = None
) -> Tuple[str, str]:
    """
    ä»çœŸå®æ•°æ®æºå‡†å¤‡ SPD è®­ç»ƒæ•°æ®
    
    æœŸæœ›çš„è¾“å…¥æ•°æ®æ ¼å¼ (JSON/Parquet):
    {
        "context": "...",           # ä¸Šä¸‹æ–‡æ–‡æœ¬
        "draft_response": "...",    # Draft Model çš„è¾“å‡º
        "target_response": "...",   # Target Model çš„è¾“å‡º
        "ground_truth": "...",      # æ­£ç¡®ç­”æ¡ˆ
        "is_correct_baseline": bool # Target Model æ˜¯å¦ç­”å¯¹
    }
    
    Args:
        config: è®­ç»ƒé…ç½®
        source_data_path: æºæ•°æ®è·¯å¾„
        tokenizer_path: Tokenizer è·¯å¾„ (ç”¨äº tokenize æ–‡æœ¬)
    
    Returns:
        train_path, val_path: å¤„ç†åçš„æ•°æ®è·¯å¾„
    """
    logger.info(f"ä»çœŸå®æ•°æ®æºåŠ è½½: {source_data_path}")
    
    # åŠ è½½ tokenizer
    if tokenizer_path is None:
        tokenizer_path = config.model_path
    
    from transformers import AutoTokenizer
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
    logger.info(f"Tokenizer åŠ è½½æˆåŠŸ: {tokenizer_path}")
    
    # åŠ è½½æºæ•°æ®
    if source_data_path.endswith('.parquet'):
        source_df = pd.read_parquet(source_data_path)
    elif source_data_path.endswith('.json') or source_data_path.endswith('.jsonl'):
        source_df = pd.read_json(source_data_path, lines=source_data_path.endswith('.jsonl'))
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {source_data_path}")
    
    logger.info(f"åŠ è½½äº† {len(source_df)} æ¡æºæ•°æ®")
    
    os.makedirs(config.data_dir, exist_ok=True)
    processed_data = []
    
    for idx, row in source_df.iterrows():
        if idx % 1000 == 0:
            logger.info(f"å¤„ç†è¿›åº¦: {idx}/{len(source_df)}")
        
        # æå–å­—æ®µ
        context = row.get('context', '')
        draft_response = row.get('draft_response', '')
        target_response = row.get('target_response', '')
        ground_truth = row.get('ground_truth', '')
        is_correct_baseline = row.get('is_correct_baseline', False)
        
        # Tokenize
        draft_tokens = tokenizer.encode(draft_response, add_special_tokens=False)
        target_tokens = tokenizer.encode(target_response, add_special_tokens=False)
        
        # å¯¹é½é•¿åº¦ (å–è¾ƒçŸ­çš„)
        min_len = min(len(draft_tokens), len(target_tokens))
        draft_tokens = draft_tokens[:min_len]
        target_tokens = target_tokens[:min_len]
        
        if min_len == 0:
            continue
        
        # æ„é€ æ ·æœ¬
        sample = {
            "data_source": "spd_scorer",
            "prompt": [
                {
                    "role": "system",
                    "content": "You are a scoring model for speculative decoding."
                },
                {
                    "role": "user",
                    "content": json.dumps({
                        "context": context,
                        "draft_tokens": draft_tokens,
                        "target_tokens": target_tokens,
                    })
                }
            ],
            "ability": "spd_scoring",
            "reward_model": {
                "style": "rule",
                "ground_truth": ground_truth,
                "draft_tokens": draft_tokens,
                "target_tokens": target_tokens,
                "is_correct_baseline": is_correct_baseline,
                "draft_len": min_len,
                "alpha": config.reward_alpha,
                "penalty_break": config.reward_penalty_break,
                "reward_correct": config.reward_correct,
                "reward_useless": config.reward_useless,
            },
            "extra_info": {
                "split": "train",
                "index": idx,
                "draft_len": min_len,
                # ä¼ é€’ç»™ Reward Function çš„å…³é”®ä¿¡æ¯
                "draft_tokens": draft_tokens,
                "target_tokens": target_tokens,
                "is_correct_baseline": is_correct_baseline,
                "alpha": config.reward_alpha,
                "penalty_break": config.reward_penalty_break,
                "reward_correct": config.reward_correct,
                "reward_useless": config.reward_useless,
                # ä¸Šä¸‹æ–‡å’Œè¡¥å…¨æœåŠ¡é…ç½®
                "context_text": context,
                "target_model_url": config.target_model_url,
                "target_model_name": config.target_model_name,
                "model_path": config.model_path,
            }
        }
        processed_data.append(sample)
    
    # ä¿å­˜
    df = pd.DataFrame(processed_data)
    train_size = int(len(df) * 0.95)
    train_df = df.iloc[:train_size]
    val_df = df.iloc[train_size:]
    
    train_path = os.path.join(config.data_dir, config.train_file)
    val_path = os.path.join(config.data_dir, config.val_file)
    
    train_df.to_parquet(train_path)
    val_df.to_parquet(val_path)
    
    logger.info(f"å¤„ç†å®Œæˆ: è®­ç»ƒé›† {len(train_df)} æ¡, éªŒè¯é›† {len(val_df)} æ¡")
    
    return train_path, val_path


# ==============================================================================
# 3. è‡ªå®šä¹‰ Reward Function
# ==============================================================================

def register_spd_reward_function():
    """
    æ³¨å†Œ SPD Scorer çš„è‡ªå®šä¹‰ Reward Function åˆ° verl
    
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨ verl å¯åŠ¨å‰è¢«è°ƒç”¨ï¼Œç¡®ä¿ reward function å¯ç”¨
    """
    
    # åˆ›å»º reward function æ–‡ä»¶
    reward_fn_code = '''
# -*- coding: utf-8 -*-
"""
SPD Scorer è‡ªå®šä¹‰ Reward Function
ç”¨äº verl æ¡†æ¶çš„ reward è®¡ç®—

å¥–åŠ±é€»è¾‘:
    - åœºæ™¯ A (åŠ é€ŸæˆåŠŸ): Baseline å¯¹ & Hybrid å¯¹ -> Reward = alpha * L
    - åœºæ™¯ B (ç ´åæ­£ç¡®): Baseline å¯¹ & Hybrid é”™ -> Reward = penalty_break
    - åœºæ™¯ C (æ— ç”¨å°è¯•): Baseline é”™ & Hybrid é”™ -> Reward = reward_useless
    - åœºæ™¯ D (çº æ­£é”™è¯¯): Baseline é”™ & Hybrid å¯¹ -> Reward = reward_correct
"""

import json
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


def compute_effective_length(accept_decisions: list) -> int:
    """
    è®¡ç®—æœ‰æ•ˆæ¥å—é•¿åº¦ L
    
    å®šä¹‰: ä»åºåˆ—å¼€å¤´è¿ç»­ä¸º True/1 çš„é•¿åº¦
    
    Args:
        accept_decisions: Accept/Reject å†³ç­–åˆ—è¡¨
    
    Returns:
        L: æœ‰æ•ˆæ¥å—é•¿åº¦
    """
    L = 0
    for decision in accept_decisions:
        if decision:
            L += 1
        else:
            break
    return L


def verify_hybrid_correctness(
    draft_tokens: list,
    target_tokens: list,
    accept_decisions: list,
    ground_truth: str
) -> bool:
    """
    éªŒè¯ Hybrid ç”Ÿæˆç»“æœçš„æ­£ç¡®æ€§
    
    Hybrid ç”Ÿæˆé€»è¾‘:
        - æ¥å—çš„ä½ç½®: ä½¿ç”¨ Draft Token
        - æ‹’ç»çš„ä½ç½®: ä½¿ç”¨ Target Token
    
    ç®€åŒ–å®ç°: 
        - è¿™é‡Œä½¿ç”¨å¯å‘å¼è§„åˆ™åˆ¤æ–­æ­£ç¡®æ€§
        - å®é™…ä½¿ç”¨æ—¶åº”è¯¥è°ƒç”¨çœŸæ­£çš„éªŒè¯å‡½æ•°
    
    Args:
        draft_tokens: Draft token IDs
        target_tokens: Target token IDs
        accept_decisions: Accept/Reject å†³ç­–
        ground_truth: æ­£ç¡®ç­”æ¡ˆ
    
    Returns:
        is_correct: Hybrid ç»“æœæ˜¯å¦æ­£ç¡®
    """
    # è®¡ç®—æœ‰æ•ˆæ¥å—é•¿åº¦
    L = compute_effective_length(accept_decisions)
    
    # æ„å»º Hybrid åºåˆ—
    # hybrid = draft[:L] + target[L:]
    hybrid_tokens = draft_tokens[:L] + target_tokens[L:]
    
    # ç®€åŒ–çš„æ­£ç¡®æ€§åˆ¤æ–­:
    # - å¦‚æœæ¥å—äº†å¤ªå¤š Mismatchï¼Œå¯èƒ½ç ´åæ­£ç¡®æ€§
    # - å®é™…ä½¿ç”¨æ—¶åº”è¯¥è§£ç å¹¶éªŒè¯ç­”æ¡ˆ
    
    mismatch_accepted = 0
    for i in range(min(L, len(draft_tokens))):
        if i < len(target_tokens) and draft_tokens[i] != target_tokens[i]:
            mismatch_accepted += 1
    
    # å¯å‘å¼: å¦‚æœæ¥å—çš„ Mismatch è¶…è¿‡ 50%ï¼Œè®¤ä¸ºå¯èƒ½å‡ºé”™
    # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„åˆ¤æ–­ï¼Œå®é™…åº”è¯¥ç”¨çœŸæ­£çš„éªŒè¯å‡½æ•°
    if L > 0 and mismatch_accepted / L > 0.5:
        return False
    
    return True


def compute_score(
    solution_str: str,
    ground_truth: str,
    draft_tokens: list = None,
    target_tokens: list = None,
    is_correct_baseline: bool = False,
    draft_len: int = 0,
    alpha: float = 1.0,
    penalty_break: float = -10.0,
    reward_correct: float = 100.0,
    reward_useless: float = 0.0,
    **kwargs
) -> float:
    """
    è®¡ç®— SPD Scorer çš„ Reward
    
    è¿™æ˜¯ verl æ¡†æ¶è°ƒç”¨çš„ä¸»å‡½æ•°
    
    Args:
        solution_str: æ¨¡å‹ç”Ÿæˆçš„ "å“åº”" (åœ¨ SPD åœºæ™¯ä¸­ï¼Œè¿™æ˜¯ Accept/Reject å†³ç­–åºåˆ—)
        ground_truth: æ­£ç¡®ç­”æ¡ˆ
        draft_tokens: Draft token IDs
        target_tokens: Target token IDs
        is_correct_baseline: Target Model æ˜¯å¦ç­”å¯¹
        draft_len: Draft åºåˆ—é•¿åº¦
        alpha: åŠ é€Ÿå¥–åŠ±ç³»æ•°
        penalty_break: ç ´åæ­£ç¡®ç­”æ¡ˆçš„æƒ©ç½š
        reward_correct: çº æ­£é”™è¯¯çš„å¥–åŠ±
        reward_useless: æ— ç”¨å°è¯•çš„å¥–åŠ±
    
    Returns:
        reward: è®¡ç®—å¾—åˆ°çš„å¥–åŠ±å€¼
    """
    try:
        # è§£ææ¨¡å‹çš„è¾“å‡º
        # åœ¨ SPD åœºæ™¯ä¸­ï¼Œæ¨¡å‹è¾“å‡ºåº”è¯¥æ˜¯ Accept/Reject å†³ç­–
        # æ ¼å¼: "1 1 1 0 1 0 ..." æˆ– "[1, 1, 1, 0, 1, 0, ...]"
        
        if solution_str.startswith('['):
            # JSON åˆ—è¡¨æ ¼å¼
            accept_decisions = json.loads(solution_str)
        else:
            # ç©ºæ ¼åˆ†éš”æ ¼å¼
            parts = solution_str.strip().split()
            accept_decisions = [int(p) > 0 for p in parts if p.isdigit()]
        
        # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å†³ç­–ï¼ˆå…¨éƒ¨æ¥å—ï¼‰
        if not accept_decisions:
            accept_decisions = [True] * draft_len
        
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        if len(accept_decisions) < draft_len:
            accept_decisions.extend([False] * (draft_len - len(accept_decisions)))
        accept_decisions = accept_decisions[:draft_len]
        
    except Exception as e:
        logger.warning(f"è§£æ Accept/Reject å†³ç­–å¤±è´¥: {e}, ä½¿ç”¨é»˜è®¤å€¼")
        accept_decisions = [True] * draft_len
    
    # è®¡ç®—æœ‰æ•ˆæ¥å—é•¿åº¦
    L = compute_effective_length(accept_decisions)
    
    # éªŒè¯ Hybrid æ­£ç¡®æ€§
    is_correct_hybrid = verify_hybrid_correctness(
        draft_tokens=draft_tokens or [],
        target_tokens=target_tokens or [],
        accept_decisions=accept_decisions,
        ground_truth=ground_truth
    )
    
    # æ ¹æ®å››åœºæ™¯è®¡ç®—å¥–åŠ±
    if is_correct_baseline and is_correct_hybrid:
        # åœºæ™¯ A: åŠ é€ŸæˆåŠŸ
        reward = alpha * L
        scenario = "A"
    elif is_correct_baseline and not is_correct_hybrid:
        # åœºæ™¯ B: ç ´åæ­£ç¡®ç­”æ¡ˆ (ä¸¥å‰æƒ©ç½š)
        reward = penalty_break
        scenario = "B"
    elif not is_correct_baseline and not is_correct_hybrid:
        # åœºæ™¯ C: æ— ç”¨å°è¯•
        reward = reward_useless
        scenario = "C"
    else:  # not is_correct_baseline and is_correct_hybrid
        # åœºæ™¯ D: çº æ­£é”™è¯¯ (é‡å¥–)
        reward = reward_correct
        scenario = "D"
    
    # è¿”å›ç»“æœ
    # verl æœŸæœ›è¿”å› float æˆ–åŒ…å« 'score' çš„ dict
    return {
        "score": reward,
        "effective_length": L,
        "scenario": scenario,
        "is_correct_hybrid": is_correct_hybrid,
        "accept_ratio": sum(accept_decisions) / len(accept_decisions) if accept_decisions else 0,
    }
'''
    
    # ç¡®ä¿ reward_score ç›®å½•å­˜åœ¨
    reward_dir = os.path.join(os.getcwd(), "verl", "utils", "reward_score")
    os.makedirs(reward_dir, exist_ok=True)
    
    # å†™å…¥ reward function æ–‡ä»¶
    reward_file = os.path.join(reward_dir, "spd_scorer_reward.py")
    with open(reward_file, 'w', encoding='utf-8') as f:
        f.write(reward_fn_code)
    
    logger.info(f"SPD Reward Function å·²å†™å…¥: {reward_file}")
    
    # ä¿®æ”¹ __init__.py ä»¥æ³¨å†Œæ–°çš„ reward function
    init_file = os.path.join(reward_dir, "__init__.py")
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»æ³¨å†Œ
    if os.path.exists(init_file):
        with open(init_file, 'r', encoding='utf-8') as f:
            init_content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å« spd_scorer
        if 'spd_scorer' not in init_content:
            # æ‰¾åˆ°åˆé€‚çš„ä½ç½®æ’å…¥
            # åœ¨ default_compute_score å‡½æ•°ä¸­æ·»åŠ  spd_scorer çš„å¤„ç†
            
            insert_code = '''
    elif data_source == "spd_scorer":
        from . import spd_scorer_reward
        res = spd_scorer_reward.compute_score(
            solution_str, 
            ground_truth,
            draft_tokens=extra_info.get('draft_tokens') if extra_info else None,
            target_tokens=extra_info.get('target_tokens') if extra_info else None,
            is_correct_baseline=extra_info.get('is_correct_baseline', False) if extra_info else False,
            draft_len=extra_info.get('draft_len', 0) if extra_info else 0,
            alpha=extra_info.get('alpha', 1.0) if extra_info else 1.0,
            penalty_break=extra_info.get('penalty_break', -10.0) if extra_info else -10.0,
            reward_correct=extra_info.get('reward_correct', 100.0) if extra_info else 100.0,
            reward_useless=extra_info.get('reward_useless', 0.0) if extra_info else 0.0,
        )
'''
            
            # åœ¨ "openai/gsm8k" æ¡ä»¶ä¹‹å‰æ’å…¥
            if 'elif data_source == "openai/gsm8k"' in init_content:
                init_content = init_content.replace(
                    'if data_source == "openai/gsm8k"',
                    f'if data_source == "spd_scorer":{insert_code[insert_code.find("from"):]}\n    elif data_source == "openai/gsm8k"'
                )
            else:
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•åœ¨å‡½æ•°å¼€å¤´æ’å…¥
                logger.warning("æ— æ³•è‡ªåŠ¨æ³¨å†Œ SPD reward functionï¼Œè¯·æ‰‹åŠ¨ä¿®æ”¹ verl/utils/reward_score/__init__.py")
            
            # å†™å›æ–‡ä»¶
            with open(init_file, 'w', encoding='utf-8') as f:
                f.write(init_content)
            
            logger.info("SPD Reward Function å·²æ³¨å†Œåˆ° verl")
    
    return reward_file


# ==============================================================================
# 4. è®­ç»ƒä¸»å‡½æ•°
# ==============================================================================

def build_training_command(config: SPDTrainingConfig, train_file: str, val_file: str) -> list:
    """
    æ„å»º verl GRPO è®­ç»ƒå‘½ä»¤
    
    Args:
        config: è®­ç»ƒé…ç½®
        train_file: è®­ç»ƒæ•°æ®è·¯å¾„
        val_file: éªŒè¯æ•°æ®è·¯å¾„
    
    Returns:
        cmd: è®­ç»ƒå‘½ä»¤åˆ—è¡¨
    """
    
    # åŸºç¡€å‘½ä»¤
    cmd = [
        sys.executable, "-m", "verl.trainer.main_ppo",
        
        # =================================================================
        # ç®—æ³•æ ¸å¿ƒé…ç½® (GRPO)
        # =================================================================
        "algorithm.adv_estimator=grpo",        # ä½¿ç”¨ GRPO ç®—æ³•
        "algorithm.use_kl_in_reward=False",    # GRPO ç‰¹æ€§
        "algorithm.kl_ctrl.kl_coef=0.001",     # KL æ•£åº¦ç³»æ•°
        
        # =================================================================
        # æ•°æ®é…ç½®
        # =================================================================
        f"data.train_files={train_file}",
        f"data.val_files={val_file}",
        f"data.train_batch_size={config.train_batch_size}",
        "data.max_prompt_length=2048",         # åŒ…å« Context + Draft + Target
        "data.max_response_length=256",        # è¾“å‡ºæ˜¯ Accept/Reject å†³ç­–åºåˆ—
        
        # =================================================================
        # æ¨¡å‹é…ç½®
        # =================================================================
        f"actor_rollout_ref.model.path={config.model_path}",
        "actor_rollout_ref.model.use_remove_padding=True",
        
        # LoRA é…ç½®
        f"actor_rollout_ref.model.lora_rank={config.lora_rank}",
        f"actor_rollout_ref.model.lora_alpha={config.lora_alpha}",
        
        # =================================================================
        # Rollout é…ç½®
        # =================================================================
        f"actor_rollout_ref.rollout.n={config.rollout_n}",
        "actor_rollout_ref.rollout.name=spd",  # ä½¿ç”¨è‡ªå®šä¹‰çš„ SPD Rollout
        f"actor_rollout_ref.rollout.gpu_memory_utilization={config.vllm_gpu_memory_utilization}",
        "actor_rollout_ref.rollout.free_cache_engine=False",
        f"actor_rollout_ref.rollout.data_parallel_size={config.n_gpus}",
        "actor_rollout_ref.rollout.enforce_eager=True",
        "actor_rollout_ref.rollout.tensor_model_parallel_size=1",
        "actor_rollout_ref.rollout.enable_chunked_prefill=False",
        "actor_rollout_ref.rollout.max_num_batched_tokens=8192",
        f"actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu={config.micro_batch_size_per_gpu}",
        
        # =================================================================
        # Actor è®­ç»ƒé…ç½®
        # =================================================================
        f"actor_rollout_ref.actor.ppo_mini_batch_size={config.ppo_mini_batch_size}",
        f"actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu={config.micro_batch_size_per_gpu}",
        "actor_rollout_ref.actor.use_kl_loss=True",
        "actor_rollout_ref.actor.kl_loss_coef=0.001",
        f"actor_rollout_ref.actor.fsdp_config.param_offload={config.offload}",
        f"actor_rollout_ref.actor.fsdp_config.optimizer_offload={config.offload}",
        
        # =================================================================
        # Reference é…ç½®
        # =================================================================
        f"actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu={config.micro_batch_size_per_gpu}",
        f"actor_rollout_ref.ref.fsdp_config.param_offload={config.offload}",
        
        # =================================================================
        # Trainer é…ç½®
        # =================================================================
        f"trainer.total_epochs={config.total_epochs}",
        f"trainer.n_gpus_per_node={config.n_gpus}",
        "trainer.nnodes=1",
        f"trainer.project_name={config.project_name}",
        f"trainer.experiment_name={config.experiment_name}",
        "trainer.test_freq=10",
        "trainer.save_freq=-1",
    ]
    
    # æ—¥å¿—é…ç½®
    if config.use_wandb:
        cmd.append("trainer.logger=['console','wandb']")
    else:
        cmd.append("trainer.logger=['console']")
    
    return cmd


def run_training(config: SPDTrainingConfig):
    """
    è¿è¡Œ SPD Scorer è®­ç»ƒ
    
    å®Œæ•´æµç¨‹:
        1. å‡†å¤‡è®­ç»ƒæ•°æ®
        2. æ³¨å†Œè‡ªå®šä¹‰ Reward Function
        3. æ„å»ºå¹¶æ‰§è¡Œè®­ç»ƒå‘½ä»¤
    
    Args:
        config: è®­ç»ƒé…ç½®
    """
    logger.info("=" * 60)
    logger.info("SPD Scorer GRPO è®­ç»ƒ")
    logger.info("=" * 60)
    
    # Step 1: å‡†å¤‡æ•°æ®
    logger.info("\n[Step 1] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    train_file, val_file = prepare_spd_training_data(config)
    
    # Step 2: æ³¨å†Œ Reward Function
    logger.info("\n[Step 2] æ³¨å†Œè‡ªå®šä¹‰ Reward Function...")
    register_spd_reward_function()
    
    # Step 3: æ„å»ºè®­ç»ƒå‘½ä»¤
    logger.info("\n[Step 3] æ„å»ºè®­ç»ƒå‘½ä»¤...")
    cmd = build_training_command(config, train_file, val_file)
    
    logger.info("\nè®­ç»ƒå‘½ä»¤:")
    logger.info(" ".join(cmd[:5]) + " \\")
    for arg in cmd[5:]:
        logger.info(f"    {arg} \\")
    
    # Step 4: æ‰§è¡Œè®­ç»ƒ
    logger.info("\n[Step 4] å¯åŠ¨è®­ç»ƒ...")
    logger.info("=" * 60)
    
    offload_status = "ON" if config.offload else "OFF"
    logger.info(f"ğŸš€ å¼€å§‹ SPD Scorer GRPO è®­ç»ƒ")
    logger.info(f"é…ç½®: {config.n_gpus} GPU | Batch={config.train_batch_size} | Rollout N={config.rollout_n} | Offload={offload_status}")
    logger.info("=" * 60)
    
    try:
        env = os.environ.copy()
        env["HYDRA_FULL_ERROR"] = "1"
        env["NCCL_P2P_DISABLE"] = "1"
        
        subprocess.run(cmd, check=True, env=env)
        
    except subprocess.CalledProcessError as e:
        logger.error(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise
    except KeyboardInterrupt:
        logger.info("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ã€‚")


# ==============================================================================
# 5. ç‹¬ç«‹çš„ SPD Scorer è®­ç»ƒå¾ªç¯ (ä¸ä¾èµ– verl çš„ main_ppo)
# ==============================================================================

def train_spd_scorer_standalone(config: SPDTrainingConfig):
    """
    ç‹¬ç«‹çš„ SPD Scorer è®­ç»ƒå‡½æ•°
    
    è¿™ä¸ªå‡½æ•°æä¾›äº†ä¸€ä¸ªä¸å®Œå…¨ä¾èµ– verl.trainer.main_ppo çš„è®­ç»ƒé€‰é¡¹ã€‚
    å®ƒç›´æ¥ä½¿ç”¨ verl çš„åº•å±‚ç»„ä»¶ (FSDP Engine, Optimizer ç­‰) æ¥è®­ç»ƒ SPD Scorerã€‚
    
    é€‚ç”¨åœºæ™¯:
        - éœ€è¦æ›´ç²¾ç»†åœ°æ§åˆ¶è®­ç»ƒæµç¨‹
        - SPD Scorer çš„è¾“å‡ºæ ¼å¼ä¸æ ‡å‡† LLM å·®å¼‚è¾ƒå¤§
        - éœ€è¦è‡ªå®šä¹‰ rollout é€»è¾‘
    
    Args:
        config: è®­ç»ƒé…ç½®
    """
    logger.info("=" * 60)
    logger.info("SPD Scorer ç‹¬ç«‹è®­ç»ƒæ¨¡å¼")
    logger.info("=" * 60)
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    try:
        import torch
        import torch.distributed as dist
        from torch.utils.data import DataLoader
        
        # å¯¼å…¥ SPD Scorer
        from spd_scorer import ScoringActor, ScoringModelConfig, SPDRewardFunction
        
        logger.info("æˆåŠŸå¯¼å…¥ SPD Scorer æ¨¡å—")
    except ImportError as e:
        logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        logger.info("è¯·ç¡®ä¿ spd_scorer.py åœ¨å½“å‰ç›®å½•")
        return
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (å¦‚æœéœ€è¦)
    if not dist.is_initialized():
        # å•æœºè®­ç»ƒæ—¶ï¼Œä½¿ç”¨ç®€å•çš„åˆå§‹åŒ–
        if torch.cuda.is_available():
            dist.init_process_group(backend='nccl', init_method='env://')
        else:
            logger.warning("CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è®­ç»ƒ")
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = ScoringModelConfig(
        model_name_or_path=config.model_path,
        lora_rank=config.lora_rank,
        lora_alpha=config.lora_alpha,
        target_modules=config.lora_target_modules,
    )
    
    # åˆ›å»ºå¥–åŠ±å‡½æ•°
    reward_fn = SPDRewardFunction(
        alpha=config.reward_alpha,
        penalty_break=config.reward_penalty_break,
        reward_correct=config.reward_correct,
        reward_useless=config.reward_useless,
    )
    
    logger.info("\næ¨¡å‹é…ç½®:")
    logger.info(f"  - åŸºç¡€æ¨¡å‹: {config.model_path}")
    logger.info(f"  - LoRA Rank: {config.lora_rank}")
    logger.info(f"  - LoRA Alpha: {config.lora_alpha}")
    
    logger.info("\nå¥–åŠ±é…ç½®:")
    logger.info(f"  - Alpha (åœºæ™¯A): {config.reward_alpha}")
    logger.info(f"  - Penalty Break (åœºæ™¯B): {config.reward_penalty_break}")
    logger.info(f"  - Reward Correct (åœºæ™¯D): {config.reward_correct}")
    logger.info(f"  - Reward Useless (åœºæ™¯C): {config.reward_useless}")
    
    # è¿™é‡Œåªæ˜¯ä¸€ä¸ªæ¡†æ¶ç¤ºä¾‹
    # å®Œæ•´å®ç°éœ€è¦:
    # 1. åŠ è½½æ•°æ®
    # 2. åˆå§‹åŒ–æ¨¡å‹ (ScoringActor)
    # 3. å®ç° GRPO è®­ç»ƒå¾ªç¯
    # 4. ä¿å­˜æ¨¡å‹
    
    logger.info("\n[æ³¨æ„] ç‹¬ç«‹è®­ç»ƒæ¨¡å¼éœ€è¦æ›´å¤šå®ç°å·¥ä½œ")
    logger.info("å»ºè®®å…ˆä½¿ç”¨ verl é›†æˆæ¨¡å¼ (run_training)")
    logger.info("å¦‚æœéœ€è¦å®Œæ•´çš„ç‹¬ç«‹è®­ç»ƒå¾ªç¯ï¼Œè¯·å‚è€ƒ verl çš„ trainer å®ç°")


# ==============================================================================
# 6. å‘½ä»¤è¡Œå…¥å£
# ==============================================================================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SPD Scorer GRPO è®­ç»ƒ")
    
    parser.add_argument("--mode", type=str, default="verl", 
                        choices=["verl", "standalone"],
                        help="è®­ç»ƒæ¨¡å¼: verl (ä½¿ç”¨ verl æ¡†æ¶) æˆ– standalone (ç‹¬ç«‹è®­ç»ƒ)")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_path", type=str, default="meta-llama/Llama-3-8B",
                        help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_rank", type=int, default=16, help="LoRA Rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA Alpha")
    
    # æ•°æ®é…ç½®
    parser.add_argument("--data_dir", type=str, default="data/spd_scorer",
                        help="æ•°æ®ç›®å½•")
    parser.add_argument("--num_samples", type=int, default=10000,
                        help="ç”Ÿæˆçš„æ¨¡æ‹Ÿæ ·æœ¬æ•°é‡")
    parser.add_argument("--source_data", type=str, default=None,
                        help="çœŸå®æ•°æ®æºè·¯å¾„ (å¯é€‰)")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--n_gpus", type=int, default=8, help="GPU æ•°é‡")
    parser.add_argument("--train_batch_size", type=int, default=64, help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--rollout_n", type=int, default=8, help="GRPO Rollout N")
    parser.add_argument("--total_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    
    # å¥–åŠ±é…ç½®
    parser.add_argument("--reward_alpha", type=float, default=1.0, help="åœºæ™¯Aå¥–åŠ±ç³»æ•°")
    parser.add_argument("--reward_penalty_break", type=float, default=-10.0, help="åœºæ™¯Bæƒ©ç½š")
    parser.add_argument("--reward_correct", type=float, default=100.0, help="åœºæ™¯Då¥–åŠ±")
    parser.add_argument("--reward_useless", type=float, default=0.0, help="åœºæ™¯Cå¥–åŠ±")
    
    # è¡¥å…¨æœåŠ¡
    parser.add_argument("--target_model_url", type=str, default=None,
                        help="Target Model vLLM API åœ°å€ (e.g. http://localhost:8000/v1/completions)")
    parser.add_argument("--target_model_name", type=str, default="target-model",
                        help="Target Model åç§°")
    
    # å…¶ä»–
    parser.add_argument("--no_wandb", action="store_true", help="ç¦ç”¨ WandB")
    parser.add_argument("--project_name", type=str, default="verl_spd_scorer",
                        help="WandB é¡¹ç›®å")
    parser.add_argument("--experiment_name", type=str, default="spd_grpo_training",
                        help="å®éªŒå")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = SPDTrainingConfig(
        model_path=args.model_path,
        data_dir=args.data_dir,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        n_gpus=args.n_gpus,
        train_batch_size=args.train_batch_size,
        rollout_n=args.rollout_n,
        total_epochs=args.total_epochs,
        reward_alpha=args.reward_alpha,
        reward_penalty_break=args.reward_penalty_break,
        reward_correct=args.reward_correct,
        reward_useless=args.reward_useless,
        target_model_url=args.target_model_url,
        target_model_name=args.target_model_name,
        use_wandb=not args.no_wandb,
        project_name=args.project_name,
        experiment_name=args.experiment_name,
    )
    
    # æ‰“å°é…ç½®
    logger.info("\n" + "=" * 60)
    logger.info("SPD Scorer è®­ç»ƒé…ç½®")
    logger.info("=" * 60)
    logger.info(f"æ¨¡å¼: {args.mode}")
    logger.info(f"æ¨¡å‹: {config.model_path}")
    logger.info(f"LoRA: rank={config.lora_rank}, alpha={config.lora_alpha}")
    logger.info(f"è®­ç»ƒ: {config.n_gpus} GPU, batch={config.train_batch_size}, epochs={config.total_epochs}")
    logger.info(f"GRPO: rollout_n={config.rollout_n}")
    logger.info(f"å¥–åŠ±: A={config.reward_alpha}*L, B={config.reward_penalty_break}, C={config.reward_useless}, D={config.reward_correct}")
    logger.info("=" * 60 + "\n")
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©è®­ç»ƒæ–¹æ³•
    if args.mode == "verl":
        run_training(config)
    else:
        train_spd_scorer_standalone(config)


if __name__ == "__main__":
    main()

