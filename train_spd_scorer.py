# -*- coding: utf-8 -*-
"""
Speculative Decoding Scorer è®­ç»ƒè„šæœ¬
ä½¿ç”¨ verl æ¡†æ¶çš„ GRPO ç®—æ³•è®­ç»ƒ SPD Scoring Model

æ ¸å¿ƒæ€æƒ³:
    - å°† SPD Scorer çš„ Accept/Reject å†³ç­–å»ºæ¨¡ä¸ºä¸€ç§ç‰¹æ®Šçš„"åºåˆ—ç”Ÿæˆ"ä»»åŠ¡
    - è¾“å…¥: [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
    - è¾“å‡º: å¯¹æ¯ä¸ª Draft Token ä½ç½®çš„ Accept/Reject å†³ç­–
    - Reward: åŸºäºå››åœºæ™¯é€»è¾‘è®¡ç®— (åŠ é€ŸæˆåŠŸ/ç ´åæ­£ç¡®/æ— ç”¨å°è¯•/çº æ­£é”™è¯¯)

ä½œè€…: AI Assistant
æ—¥æœŸ: 2025-11-25
"""

import os
import sys
import subprocess
import logging
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass, field

import torch
import pandas as pd

from transformers import AutoTokenizer

# ç¡®ä¿å½“å‰ç›®å½•åœ¨ sys.path ä¸­
sys.path.insert(0, os.getcwd())

# ==============================================================================
# Monkey Patch: æ³¨å†Œ SPD Scorer æ¨¡å‹
# ==============================================================================
import verl.utils.model
from spd_scorer import AutoModelForSPDScoring

# å¼ºåˆ¶æ³¨å…¥è‡ªå®šä¹‰çš„æ¨¡å‹åŠ è½½é€»è¾‘
_original_create_huggingface_actor = verl.utils.model.create_huggingface_actor

def _patched_create_huggingface_actor(model_name: str, override_config_kwargs=None, automodel_kwargs=None) -> torch.nn.Module:
    """
    Hook åçš„ create_huggingface_actor å‡½æ•°ï¼Œæ‹¦æˆªå¹¶åŠ è½½ SPD Scorer
    """
    if override_config_kwargs is None:
        override_config_kwargs = {}
    if automodel_kwargs is None:
        automodel_kwargs = {}
        
    logger.info(f"[Patch] Intercepting model loading for: {model_name}")
    logger.info(f"[Patch] Loading AutoModelForSPDScoring...")
        
    # è·å– HF Config
    module_config = verl.utils.model.get_huggingface_actor_config(
        model_name, override_config_kwargs, trust_remote_code=automodel_kwargs.get("trust_remote_code", False)
    )
    
    # ä½¿ç”¨ SPD Scorer Factory åŠ è½½æ¨¡å‹
    # æ³¨æ„: è¿™é‡Œä¼šè°ƒç”¨ ScoringActor çš„åˆå§‹åŒ–ï¼Œå†…éƒ¨å¯èƒ½ä¼šå†æ¬¡åŠ è½½ Backbone
    # ä½†ç”±äº vLLM å’Œ HF çš„ç¼“å­˜æœºåˆ¶ï¼Œæˆ–è€…å•çº¯çš„å¤šæ¬¡åŠ è½½ï¼Œåªè¦æ˜¾å­˜è¶³å¤Ÿï¼Œæ˜¯å¯ä»¥æ¥å—çš„
    model = AutoModelForSPDScoring.from_config(module_config, **automodel_kwargs)
    
    return model

# åº”ç”¨ Patch: æ›¿æ¢ verl.utils.model ä¸­çš„å‡½æ•°
verl.utils.model.create_huggingface_actor = _patched_create_huggingface_actor
logger.info("âœ… å·²åº”ç”¨ Monkey Patch: verl.utils.model.create_huggingface_actor -> AutoModelForSPDScoring")

# ==============================================================================

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ==============================================================================
# 1. é…ç½®å®šä¹‰
# ==============================================================================

@dataclass
class SPDTrainingConfig:
    """SPD Scorer è®­ç»ƒé…ç½®"""
    
    # æ¨¡å‹è·¯å¾„
    model_path: str = "meta-llama/Llama-3-8B"
    
    # æ•°æ®è·¯å¾„
    data_dir: str = "data/spd_scorer"
    train_file: str = "train.parquet"
    val_file: str = "val.parquet"
    
    # LoRA é…ç½®
    lora_rank: int = 16
    lora_alpha: int = 32
    lora_target_modules: List[str] = field(default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj"])
    
    # è®­ç»ƒè¶…å‚æ•°
    n_gpus: int = 8
    train_batch_size: int = 64
    ppo_mini_batch_size: int = 32
    micro_batch_size_per_gpu: int = 4
    rollout_n: int = 8  # GRPO: æ¯ä¸ªæ ·æœ¬é‡‡æ · N ä¸ªå†³ç­–åºåˆ—
    total_epochs: int = 3
    
    # å¥–åŠ±ç³»æ•°
    reward_alpha: float = 1.0          # åœºæ™¯ A: alpha * L
    reward_penalty_break: float = -10.0  # åœºæ™¯ B: ç ´åæ­£ç¡®ç­”æ¡ˆ
    reward_correct: float = 100.0       # åœºæ™¯ D: çº æ­£é”™è¯¯
    reward_useless: float = 0.0         # åœºæ™¯ C: æ— ç”¨å°è¯•
    
    # ç‰¹æ®Š Token
    sep_token: str = "<|sep|>"
    sep_token_id: int = 128009  # Llama-3 çš„ <|eot_id|>ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    
    # è¡¥å…¨æœåŠ¡é…ç½®
    target_model_url: Optional[str] = None  # e.g. "http://localhost:8000/v1/completions"
    target_model_name: str = "target-model"
    
    # å…¶ä»–é…ç½®
    vllm_gpu_memory_utilization: float = 0.7
    offload: bool = False
    use_wandb: bool = True
    project_name: str = "verl_spd_scorer"
    experiment_name: str = "spd_grpo_training"


# ==============================================================================
# 2. æ•°æ®å‡†å¤‡
# ==============================================================================
def prepare_spd_data_from_real_source(
    config: SPDTrainingConfig,
    source_data_path: List[str],
) -> Tuple[str, str]:
    """
    ä»çœŸå®æ•°æ®æº (SPDç”Ÿæˆæ•°æ® + Metadata) å‡†å¤‡ SPD è®­ç»ƒæ•°æ®
    
    Args:
        config: è®­ç»ƒé…ç½®
        source_data_path: åŒ…å«ä¸¤ä¸ªæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ [spd_gen_data_file, metadata_file]
    """
    if len(source_data_path) != 2:
        raise ValueError("source_data_path å¿…é¡»æ˜¯åŒ…å«ä¸¤ä¸ªå…ƒç´ çš„åˆ—è¡¨: [spd_gen_data_file, metadata_file]")
    
    spd_gen_data_file = source_data_path[0]
    meta_file = source_data_path[1]
    logger.info(f"SPD Gen Data File: {spd_gen_data_file}")
    logger.info(f"Metadata File: {meta_file}")
    
    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(config.model_path, trust_remote_code=True)
    logger.info(f"Tokenizer åŠ è½½æˆåŠŸ: {config.model_path}")
    
    # 1. åŠ è½½ Metadata åˆ°å†…å­˜å­—å…¸ (Index -> Data)
    logger.info("åŠ è½½ Metadata...")
    meta_df = pd.read_json(meta_file, lines=True)
    # index å­—æ®µå³ä¸º sample_idx
    meta_dict = meta_df.set_index("index").to_dict(orient="index")
    logger.info(f"Metadata æ•°æ®é‡: {len(meta_dict)}")
    
    # 2. éå† SPD ç”Ÿæˆæ•°æ®å¹¶å¤„ç†
    logger.info("å¤„ç† SPD ç”Ÿæˆæ•°æ®...")
    spd_gen_df = pd.read_json(spd_gen_data_file, lines=True)
    logger.info(f"SPD ç”Ÿæˆæ•°æ®é‡: {len(spd_gen_df)}")
    
    os.makedirs(config.data_dir, exist_ok=True)
    processed_data = []
    
    for idx, row in spd_gen_df.iterrows():
        if idx % 1000 == 0:
            logger.info(f"å¤„ç†è¿›åº¦: {idx}/{len(spd_gen_df)}")
            
        sample_idx = row.get('sample_idx')
        cut_idx = row.get('cut_idx')
        
        # æŸ¥æ‰¾ Metadata
        if sample_idx not in meta_dict:
            logger.warning(f"Sample {sample_idx} not found in metadata. Skipping.")
            continue
            
        meta = meta_dict[sample_idx]
        
        problem = meta.get('problem')
        ground_truth = meta.get('reference_answer')
        is_correct_baseline = meta.get('is_correct')
        
        # æ„é€  Context IDs
        # Step A: Base Chat (System + User)
        base_messages = [
            {
                "role": "system",
                "content": "Please reason step by step, and put your final answer within \\boxed{}.",
            },
            {
                "role": "user",
                "content": problem,
            },
        ]
        
        # apply_chat_template è¿”å› tensor if return_tensors='pt'
        # è¿™é‡Œæˆ‘ä»¬éœ€è¦ list[int] ä»¥ä¾¿æ‹¼æ¥
        base_ids = tokenizer.apply_chat_template(
            base_messages,
            tokenize=True,
            add_generation_prompt=True
        )
        
        # Step B: Answer Prefix
        full_answer_ids = meta.get('answer_ids')
        answer_prefix_ids = full_answer_ids[:cut_idx]
        
        # Step C: Splice Context
        context_ids = base_ids + answer_prefix_ids
        
        # è·å– Draft / Target IDs
        draft_ids = row.get('draft_output_ids')
        target_ids = row.get('target_output_ids')
        
        # ä¸¥æ ¼æ ¡éªŒé•¿åº¦: å¦‚æœ Draft å’Œ Target é•¿åº¦ä¸ä¸€è‡´ï¼Œç›´æ¥è·³è¿‡
        if len(draft_ids) != len(target_ids):
            logger.warning(f"Sample {sample_idx} draft/target length mismatch ({len(draft_ids)} vs {len(target_ids)}). Skipping.")
            continue
        if len(draft_ids) == 0:
            logger.warning(f"Sample {sample_idx} draft length is 0. Skipping.")
            continue
            
        draft_len = len(draft_ids)
        target_len = len(target_ids) # åº”è¯¥ç­‰äº draft_len

        # =================================================================
        # æ„é€ å®Œæ•´çš„ Input IDs (ç”¨äº Actor è¾“å…¥)
        # ç»“æ„: [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
        # =================================================================
        
        # è·å– SEP Token ID (Llama-3 eot_id)
        sep_token_id = config.sep_token_id
        
        # æ‹¼æ¥
        # æ³¨æ„: è¿™é‡Œå‡è®¾ draft_ids å’Œ target_ids å·²ç»æ˜¯ list[int]
        full_input_ids = (
            context_ids + 
            [sep_token_id] + 
            draft_ids + 
            [sep_token_id] + 
            target_ids[:-1] + 
            [sep_token_id]
        )
        
        # è®¡ç®—å…³é”®ä½ç½®ç´¢å¼• (ç”¨äºåç»­ Mask ç”Ÿæˆå’Œé€»è¾‘å¤„ç†)
        # draft_start_idx: Draft Tokens çš„èµ·å§‹ä½ç½® (åŒ…å«å‰é¢çš„ SEP)
        # å®é™…ä¸Šåœ¨ list ç´¢å¼•ä¸­ï¼Œdraft_start_idx æŒ‡å‘çš„æ˜¯ Draft çš„ç¬¬ä¸€ä¸ª Token
        # context_len (åŒ…å« SEP) = len(context_ids) + 1
        draft_start_idx = len(context_ids) + 1 
        
        # draft_end_idx: Draft Tokens çš„ç»“æŸä½ç½® (ä¸åŒ…å«åé¢çš„ SEP)
        draft_end_idx = draft_start_idx + draft_len
        
        # target_start_idx: Target Tokens çš„èµ·å§‹ä½ç½®
        # å‰é¢æœ‰: Context + SEP + Draft + SEP
        target_start_idx = draft_end_idx + 1
        
        # target_end_idx: Target Tokens çš„ç»“æŸä½ç½®
        target_end_idx = target_start_idx + target_len
            
        # æ„é€ æ ·æœ¬
        sample = {
            "data_source": "spd_scorer",
            
            # (1) ä¸ºäº†å…¼å®¹ verl Dataset æ¥å£ï¼Œè¿™é‡Œæ”¾ä¸€ä¸ª dummy prompt (å®é™…ä¸Šæˆ‘ä»¬çš„ rollout/model åº”è¯¥ç›´æ¥è¯»å– input_ids)
            "prompt": "dummy_prompt", 
            
            # =====================================================
            # æ ¸å¿ƒæ•°æ® (é¡¶å±‚å­—æ®µï¼Œæ–¹ä¾¿åç»­ Rollout/Training ç›´æ¥è¯»å–)
            # =====================================================
            "input_ids": full_input_ids,    # [Context] + [SEP] + [Draft] + [SEP] + [Target] + [SEP]
            
            # =====================================================
            # verl å…¼å®¹å­—æ®µ
            # =====================================================
            "ability": "spd_scoring",
            "reward_model": {
                "style": "rule",
                # (2) å°† Ground Truth æ”¾å…¥ reward_model å­—æ®µï¼Œè¿™æ˜¯ verl çš„æƒ¯ä¾‹
                "ground_truth": ground_truth,
            },
            "extra_info": {
                "split": "train",
                "index": idx,
                "context_ids": context_ids,
                "draft_tokens": draft_ids,
                "target_tokens": target_ids[:-1],
                "bonus_tokens": target_ids[-1:],
                "is_correct_baseline": is_correct_baseline,
                "draft_len": draft_len,
                # ä½ç½®ä¿¡æ¯ (Non-Tensor, ä½†å¯ä»¥åœ¨ Rollout ä¸­è½¬ä¸º Tensor)
                "draft_start_idx": draft_start_idx,
                "draft_end_idx": draft_end_idx,
                "target_start_idx": target_start_idx,
                "target_end_idx": target_end_idx,
            }
        }
        processed_data.append(sample)
    
    # ä¿å­˜
    df = pd.DataFrame(processed_data)
    train_size = int(len(df) * 0.95)
    train_df = df.iloc[:train_size]
    val_df = df.iloc[train_size:]
    
    train_path = os.path.join(config.data_dir, config.train_file)
    val_path = os.path.join(config.data_dir, config.val_file)
    
    train_df.to_parquet(train_path)
    val_df.to_parquet(val_path)
    
    logger.info(f"å¤„ç†å®Œæˆ: è®­ç»ƒé›† {len(train_df)} æ¡, éªŒè¯é›† {len(val_df)} æ¡")
    
    return train_path, val_path


# ==============================================================================
# 3. è®­ç»ƒä¸»å‡½æ•°
# ==============================================================================

def build_training_command(config: SPDTrainingConfig, train_file: str, val_file: str) -> list:
    """
    æ„å»º verl GRPO è®­ç»ƒå‘½ä»¤
    
    Args:
        config: è®­ç»ƒé…ç½®
        train_file: è®­ç»ƒæ•°æ®è·¯å¾„
        val_file: éªŒè¯æ•°æ®è·¯å¾„
    
    Returns:
        cmd: è®­ç»ƒå‘½ä»¤åˆ—è¡¨
    """
    
    # åŸºç¡€å‘½ä»¤
    cmd = [
        sys.executable, "-m", "verl.trainer.main_ppo",
        
        # =================================================================
        # ç®—æ³•æ ¸å¿ƒé…ç½® (GRPO)
        # =================================================================
        "algorithm.adv_estimator=grpo",        # ä½¿ç”¨ GRPO ç®—æ³•
        "algorithm.use_kl_in_reward=False",    # GRPO ç‰¹æ€§
        "algorithm.kl_ctrl.kl_coef=0.001",     # KL æ•£åº¦ç³»æ•°
        
        # =================================================================
        # æ•°æ®é…ç½®
        # =================================================================
        f"data.train_files={train_file}",
        f"data.val_files={val_file}",
        
        # ä½¿ç”¨è‡ªå®šä¹‰çš„ SPD Dataset
        "data.custom_cls.path=verl.utils.dataset.spd_dataset",
        "data.custom_cls.name=SPDRLHFDataset",
        
        f"data.train_batch_size={config.train_batch_size}",
        "data.max_prompt_length=2048",         # åŒ…å« Context + Draft + Target
        "data.max_response_length=256",        # è¾“å‡ºæ˜¯ Accept/Reject å†³ç­–åºåˆ—
        
        # =================================================================
        # æ¨¡å‹é…ç½®
        # =================================================================
        f"actor_rollout_ref.model.path={config.model_path}",
        "actor_rollout_ref.model.use_remove_padding=True",
        
        # LoRA é…ç½®
        f"actor_rollout_ref.model.lora_rank={config.lora_rank}",
        f"actor_rollout_ref.model.lora_alpha={config.lora_alpha}",
        
        # =================================================================
        # Rollout é…ç½®
        # =================================================================
        f"actor_rollout_ref.rollout.n={config.rollout_n}",
        "actor_rollout_ref.rollout.name=spd",  # ä½¿ç”¨è‡ªå®šä¹‰çš„ SPD Rollout
        f"actor_rollout_ref.rollout.gpu_memory_utilization={config.vllm_gpu_memory_utilization}",
        "actor_rollout_ref.rollout.free_cache_engine=False",
        f"actor_rollout_ref.rollout.data_parallel_size={config.n_gpus}",
        "actor_rollout_ref.rollout.enforce_eager=True",
        "actor_rollout_ref.rollout.tensor_model_parallel_size=1",
        "actor_rollout_ref.rollout.enable_chunked_prefill=False",
        "actor_rollout_ref.rollout.max_num_batched_tokens=8192",
        f"actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu={config.micro_batch_size_per_gpu}",
        
        # =================================================================
        # Actor è®­ç»ƒé…ç½®
        # =================================================================
        f"actor_rollout_ref.actor.ppo_mini_batch_size={config.ppo_mini_batch_size}",
        f"actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu={config.micro_batch_size_per_gpu}",
        "actor_rollout_ref.actor.use_kl_loss=True",
        "actor_rollout_ref.actor.kl_loss_coef=0.001",
        f"actor_rollout_ref.actor.fsdp_config.param_offload={config.offload}",
        f"actor_rollout_ref.actor.fsdp_config.optimizer_offload={config.offload}",
        
        # =================================================================
        # Reference é…ç½®
        # =================================================================
        f"actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu={config.micro_batch_size_per_gpu}",
        f"actor_rollout_ref.ref.fsdp_config.param_offload={config.offload}",
        
        # =================================================================
        # Trainer é…ç½®
        # =================================================================
        f"trainer.total_epochs={config.total_epochs}",
        f"trainer.n_gpus_per_node={config.n_gpus}",
        "trainer.nnodes=1",
        f"trainer.project_name={config.project_name}",
        f"trainer.experiment_name={config.experiment_name}",
        "trainer.test_freq=10",
        "trainer.save_freq=-1",
    ]
    
    # æ—¥å¿—é…ç½®
    if config.use_wandb:
        cmd.append("trainer.logger=['console','wandb']")
    else:
        cmd.append("trainer.logger=['console']")
    
    return cmd


def run_training(config: SPDTrainingConfig, source_data_path: List[str]):
    """
    è¿è¡Œ SPD Scorer è®­ç»ƒ
    
    å®Œæ•´æµç¨‹:
        1. å‡†å¤‡è®­ç»ƒæ•°æ®
        2. æ„å»ºå¹¶æ‰§è¡Œè®­ç»ƒå‘½ä»¤
    
    Args:
        config: è®­ç»ƒé…ç½®
        source_data_path: åŒ…å«ä¸¤ä¸ªæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ [spd_gen_data_file, metadata_file]
    """
    logger.info("=" * 60)
    logger.info("SPD Scorer GRPO è®­ç»ƒ")
    logger.info("=" * 60)
    
    # Step 1: å‡†å¤‡æ•°æ®
    logger.info("\n[Step 1] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    train_file, val_file = prepare_spd_data_from_real_source(config, source_data_path)
    
    # Step 2: æ„å»ºè®­ç»ƒå‘½ä»¤
    logger.info("\n[Step 2] æ„å»ºè®­ç»ƒå‘½ä»¤...")
    cmd = build_training_command(config, train_file, val_file)
    
    logger.info("\nè®­ç»ƒå‘½ä»¤:")
    logger.info(" ".join(cmd[:5]) + " \\")
    for arg in cmd[5:]:
        logger.info(f"    {arg} \\")
    
    # Step 3: æ‰§è¡Œè®­ç»ƒ
    logger.info("\n[Step 3] å¯åŠ¨è®­ç»ƒ...")
    logger.info("=" * 60)
    
    offload_status = "ON" if config.offload else "OFF"
    logger.info(f"ğŸš€ å¼€å§‹ SPD Scorer GRPO è®­ç»ƒ")
    logger.info(f"é…ç½®: {config.n_gpus} GPU | Batch={config.train_batch_size} | Rollout N={config.rollout_n} | Offload={offload_status}")
    logger.info("=" * 60)
    
    try:
        # è·å–åŒ…å«å¥–åŠ±é…ç½®çš„ç¯å¢ƒå˜é‡
        env = _get_reward_config_env(config)
        
        # [NEW] å°†æ¨¡å‹é…ç½®ä¹Ÿæ³¨å…¥ç¯å¢ƒå˜é‡ï¼Œä¾› spd_scorer.py è¯»å–
        env["SPD_MODEL_PATH"] = str(config.model_path)
        env["SPD_LORA_RANK"] = str(config.lora_rank)
        env["SPD_LORA_ALPHA"] = str(config.lora_alpha)
        
        env["HYDRA_FULL_ERROR"] = "1"
        env["NCCL_P2P_DISABLE"] = "1"
        
        subprocess.run(cmd, check=True, env=env)
        
    except subprocess.CalledProcessError as e:
        logger.error(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        raise
    except KeyboardInterrupt:
        logger.info("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ã€‚")


def _get_reward_config_env(config: SPDTrainingConfig) -> Dict[str, str]:
    """ç”Ÿæˆ Reward Function æ‰€éœ€çš„ç¯å¢ƒå˜é‡"""
    env = os.environ.copy()
    env["SPD_REWARD_ALPHA"] = str(config.reward_alpha)
    env["SPD_REWARD_PENALTY_BREAK"] = str(config.reward_penalty_break)
    env["SPD_REWARD_CORRECT"] = str(config.reward_correct)
    env["SPD_REWARD_USELESS"] = str(config.reward_useless)
    if config.target_model_url:
        env["SPD_TARGET_MODEL_URL"] = str(config.target_model_url)
    if config.target_model_name:
        env["SPD_TARGET_MODEL_NAME"] = str(config.target_model_name)
    return env


# ==============================================================================
# 4. ç‹¬ç«‹çš„ SPD Scorer è®­ç»ƒå¾ªç¯ (ä¸ä¾èµ– verl çš„ main_ppo)
# ==============================================================================

def train_spd_scorer_standalone(config: SPDTrainingConfig):
    """
    ç‹¬ç«‹çš„ SPD Scorer è®­ç»ƒå‡½æ•°
    
    è¿™ä¸ªå‡½æ•°æä¾›äº†ä¸€ä¸ªä¸å®Œå…¨ä¾èµ– verl.trainer.main_ppo çš„è®­ç»ƒé€‰é¡¹ã€‚
    å®ƒç›´æ¥ä½¿ç”¨ verl çš„åº•å±‚ç»„ä»¶ (FSDP Engine, Optimizer ç­‰) æ¥è®­ç»ƒ SPD Scorerã€‚
    
    é€‚ç”¨åœºæ™¯:
        - éœ€è¦æ›´ç²¾ç»†åœ°æ§åˆ¶è®­ç»ƒæµç¨‹
        - SPD Scorer çš„è¾“å‡ºæ ¼å¼ä¸æ ‡å‡† LLM å·®å¼‚è¾ƒå¤§
        - éœ€è¦è‡ªå®šä¹‰ rollout é€»è¾‘
    
    Args:
        config: è®­ç»ƒé…ç½®
    """
    logger.info("=" * 60)
    logger.info("SPD Scorer ç‹¬ç«‹è®­ç»ƒæ¨¡å¼")
    logger.info("=" * 60)
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    try:
        import torch
        import torch.distributed as dist
        from torch.utils.data import DataLoader
        
        # å¯¼å…¥ SPD Scorer
        from spd_scorer import ScoringActor, ScoringModelConfig, SPDRewardFunction
        
        logger.info("æˆåŠŸå¯¼å…¥ SPD Scorer æ¨¡å—")
    except ImportError as e:
        logger.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        logger.info("è¯·ç¡®ä¿ spd_scorer.py åœ¨å½“å‰ç›®å½•")
        return
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (å¦‚æœéœ€è¦)
    if not dist.is_initialized():
        # å•æœºè®­ç»ƒæ—¶ï¼Œä½¿ç”¨ç®€å•çš„åˆå§‹åŒ–
        if torch.cuda.is_available():
            dist.init_process_group(backend='nccl', init_method='env://')
        else:
            logger.warning("CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è®­ç»ƒ")
    
    # åˆ›å»ºæ¨¡å‹é…ç½®
    model_config = ScoringModelConfig(
        model_name_or_path=config.model_path,
        lora_rank=config.lora_rank,
        lora_alpha=config.lora_alpha,
        target_modules=config.lora_target_modules,
    )
    
    # åˆ›å»ºå¥–åŠ±å‡½æ•°
    reward_fn = SPDRewardFunction(
        alpha=config.reward_alpha,
        penalty_break=config.reward_penalty_break,
        reward_correct=config.reward_correct,
        reward_useless=config.reward_useless,
    )
    
    logger.info("\næ¨¡å‹é…ç½®:")
    logger.info(f"  - åŸºç¡€æ¨¡å‹: {config.model_path}")
    logger.info(f"  - LoRA Rank: {config.lora_rank}")
    logger.info(f"  - LoRA Alpha: {config.lora_alpha}")
    
    logger.info("\nå¥–åŠ±é…ç½®:")
    logger.info(f"  - Alpha (åœºæ™¯A): {config.reward_alpha}")
    logger.info(f"  - Penalty Break (åœºæ™¯B): {config.reward_penalty_break}")
    logger.info(f"  - Reward Correct (åœºæ™¯D): {config.reward_correct}")
    logger.info(f"  - Reward Useless (åœºæ™¯C): {config.reward_useless}")
    
    # è¿™é‡Œåªæ˜¯ä¸€ä¸ªæ¡†æ¶ç¤ºä¾‹
    # å®Œæ•´å®ç°éœ€è¦:
    # 1. åŠ è½½æ•°æ®
    # 2. åˆå§‹åŒ–æ¨¡å‹ (ScoringActor)
    # 3. å®ç° GRPO è®­ç»ƒå¾ªç¯
    # 4. ä¿å­˜æ¨¡å‹
    
    logger.info("\n[æ³¨æ„] ç‹¬ç«‹è®­ç»ƒæ¨¡å¼éœ€è¦æ›´å¤šå®ç°å·¥ä½œ")
    logger.info("å»ºè®®å…ˆä½¿ç”¨ verl é›†æˆæ¨¡å¼ (run_training)")
    logger.info("å¦‚æœéœ€è¦å®Œæ•´çš„ç‹¬ç«‹è®­ç»ƒå¾ªç¯ï¼Œè¯·å‚è€ƒ verl çš„ trainer å®ç°")


# ==============================================================================
# 5. å‘½ä»¤è¡Œå…¥å£
# ==============================================================================

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="SPD Scorer GRPO è®­ç»ƒ")
    
    parser.add_argument("--mode", type=str, default="verl", 
                        choices=["verl", "standalone"],
                        help="è®­ç»ƒæ¨¡å¼: verl (ä½¿ç”¨ verl æ¡†æ¶) æˆ– standalone (ç‹¬ç«‹è®­ç»ƒ)")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--model_path", type=str, default="meta-llama/Llama-3-8B",
                        help="åŸºç¡€æ¨¡å‹è·¯å¾„")
    parser.add_argument("--lora_rank", type=int, default=16, help="LoRA Rank")
    parser.add_argument("--lora_alpha", type=int, default=32, help="LoRA Alpha")
    
    # æ•°æ®é…ç½®
    parser.add_argument("--data_dir", type=str, default="data/spd_scorer",
                        help="æ•°æ®ç›®å½•")
    parser.add_argument("--spd_gen_data_file", type=str, required=True,
                        help="SPD ç”Ÿæˆæ•°æ®æ–‡ä»¶è·¯å¾„ (jsonl)")
    parser.add_argument("--metadata_file", type=str, required=True,
                        help="Metadata æ–‡ä»¶è·¯å¾„ (jsonl)")
    
    # è®­ç»ƒé…ç½®
    parser.add_argument("--n_gpus", type=int, default=8, help="GPU æ•°é‡")
    parser.add_argument("--train_batch_size", type=int, default=64, help="è®­ç»ƒæ‰¹æ¬¡å¤§å°")
    parser.add_argument("--rollout_n", type=int, default=8, help="GRPO Rollout N")
    parser.add_argument("--total_epochs", type=int, default=3, help="è®­ç»ƒè½®æ•°")
    
    # å¥–åŠ±é…ç½®
    parser.add_argument("--reward_alpha", type=float, default=1.0, help="åœºæ™¯Aå¥–åŠ±ç³»æ•°")
    parser.add_argument("--reward_penalty_break", type=float, default=-10.0, help="åœºæ™¯Bæƒ©ç½š")
    parser.add_argument("--reward_correct", type=float, default=100.0, help="åœºæ™¯Då¥–åŠ±")
    parser.add_argument("--reward_useless", type=float, default=0.0, help="åœºæ™¯Cå¥–åŠ±")
    
    # è¡¥å…¨æœåŠ¡
    parser.add_argument("--target_model_url", type=str, default=None,
                        help="Target Model vLLM API åœ°å€ (e.g. http://localhost:8000/v1/completions)")
    parser.add_argument("--target_model_name", type=str, default="target-model",
                        help="Target Model åç§°")
    
    # å…¶ä»–
    parser.add_argument("--no_wandb", action="store_true", help="ç¦ç”¨ WandB")
    parser.add_argument("--project_name", type=str, default="verl_spd_scorer",
                        help="WandB é¡¹ç›®å")
    parser.add_argument("--experiment_name", type=str, default="spd_grpo_training",
                        help="å®éªŒå")
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = SPDTrainingConfig(
        model_path=args.model_path,
        data_dir=args.data_dir,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        n_gpus=args.n_gpus,
        train_batch_size=args.train_batch_size,
        rollout_n=args.rollout_n,
        total_epochs=args.total_epochs,
        reward_alpha=args.reward_alpha,
        reward_penalty_break=args.reward_penalty_break,
        reward_correct=args.reward_correct,
        reward_useless=args.reward_useless,
        target_model_url=args.target_model_url,
        target_model_name=args.target_model_name,
        use_wandb=not args.no_wandb,
        project_name=args.project_name,
        experiment_name=args.experiment_name,
    )
    
    # æ‰“å°é…ç½®
    logger.info("\n" + "=" * 60)
    logger.info("SPD Scorer è®­ç»ƒé…ç½®")
    logger.info("=" * 60)
    logger.info(f"æ¨¡å¼: {args.mode}")
    logger.info(f"æ¨¡å‹: {config.model_path}")
    logger.info(f"LoRA: rank={config.lora_rank}, alpha={config.lora_alpha}")
    logger.info(f"è®­ç»ƒ: {config.n_gpus} GPU, batch={config.train_batch_size}, epochs={config.total_epochs}")
    logger.info(f"GRPO: rollout_n={config.rollout_n}")
    logger.info(f"å¥–åŠ±: A={config.reward_alpha}*L, B={config.reward_penalty_break}, C={config.reward_useless}, D={config.reward_correct}")
    logger.info("=" * 60 + "\n")
    
    # æ ¹æ®æ¨¡å¼é€‰æ‹©è®­ç»ƒæ–¹æ³•
    source_data_path = [args.spd_gen_data_file, args.metadata_file]
    
    if args.mode == "verl":
        run_training(config, source_data_path)
    else:
        train_spd_scorer_standalone(config)


if __name__ == "__main__":
    main()

