import os
import sys
import subprocess
import datasets
import pandas as pd

# ç¡®ä¿å½“å‰ç›®å½•åœ¨sys.pathä¸­ï¼Œä»¥ä¾¿å¯¼å…¥verlåº“
sys.path.append(os.getcwd())

def extract_solution(solution_str):
    """
    ä»æ•°æ®é›†ä¸­æå–æ ‡å‡†ç­”æ¡ˆã€‚
    å¯¹äºæ•°å­¦æ¨ç†ä»»åŠ¡ï¼ˆå¦‚GSM8k, OpenR1-Mathï¼‰ï¼Œæ¨¡å‹é€šå¸¸ä¼šåœ¨ \boxed{} ä¸­è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
    è¿™ä¸ªå‡½æ•°ç”¨äºä» ground_truth å­—ç¬¦ä¸²ä¸­æå–è¿™ä¸ª boxed å†…å®¹ï¼Œä»¥ä¾¿åç»­ Reward Function è¿›è¡ŒåŒ¹é…æ‰“åˆ†ã€‚
    """
    if not solution_str:
        return ""
    
    # å¯»æ‰¾æœ€åä¸€ä¸ª \boxed{...} çš„èµ·å§‹ä½ç½®
    idx = solution_str.rfind("\\boxed")
    if idx < 0:
        return solution_str # å¦‚æœæ‰¾ä¸åˆ°ï¼Œç›´æ¥è¿”å›åŸå­—ç¬¦ä¸²ä½œä¸ºç­”æ¡ˆ
    
    # ç®€å•çš„æ‹¬å·åŒ¹é…é€»è¾‘ï¼Œæå– {} å†…éƒ¨çš„å†…å®¹
    content = solution_str[idx:]
    if content.startswith("\\boxed{"):
        count = 0
        start = 7 # len("\\boxed{")
        for i, char in enumerate(content[start:], start=start):
            if char == '{':
                count += 1
            elif char == '}':
                if count == 0:
                    return content[start:i]
                count -= 1
    
    return solution_str

def prepare_data(output_dir="data/openr1"):
    """
    å‡†å¤‡è®­ç»ƒæ•°æ®ï¼šä¸‹è½½ã€é¢„å¤„ç†å¹¶ä¿å­˜ä¸º Parquet æ ¼å¼ã€‚
    Verl æ¡†æ¶è¦æ±‚æ•°æ®æ ¼å¼ä¸º Parquetï¼Œå¹¶ä¸”åŒ…å«ç‰¹å®šçš„å­—æ®µç»“æ„ï¼ˆprompt, reward_modelç­‰ï¼‰ã€‚
    """
    print(f"æ­£åœ¨å‡†å¤‡æ•°æ®ï¼Œç›®æ ‡ç›®å½•: {output_dir}...")
    os.makedirs(output_dir, exist_ok=True)
    
    dataset_name = "open-r1/OpenR1-Math-220k"
    
    try:
        print(f"æ­£åœ¨åŠ è½½å®Œæ•´æ•°æ®é›†: {dataset_name}")
        # ä½¿ç”¨ HuggingFace datasets åº“åŠ è½½æ•°æ®
        dataset = datasets.load_dataset(dataset_name, split="train")
        print(f"æˆåŠŸåŠ è½½ {len(dataset)} æ¡æ ·æœ¬ã€‚")
    except Exception as e:
        print(f"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        return None, None

    # ç³»ç»Ÿæç¤ºè¯ï¼šå¼•å¯¼æ¨¡å‹è¿›è¡Œæ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰æ¨ç†ï¼Œå¹¶è§„èŒƒè¾“å‡ºæ ¼å¼
    system_prompt = "Please reason step by step and put your final answer within \\boxed{}."

    # å¼€å§‹æ•°æ®é¢„å¤„ç†
    print(f"å¼€å§‹å¤„ç† {len(dataset)} æ¡æ•°æ®...")
    
    processed_data = []
    for i, item in enumerate(dataset):
        if i % 10000 == 0:
            print(f"å·²å¤„ç† {i} æ¡...")
        
        # é€‚é…ä¸åŒçš„æ•°æ®é›†å­—æ®µå
        q = item.get('problem', item.get('question'))
        a = item.get('solution', item.get('response'))
        
        if not q or not a: continue

        # æ„é€ ç¬¦åˆ Verl åè®®çš„æ•°æ®ç»“æ„
        processed_data.append({
            # data_source æŒ‡å®šäº†ä½¿ç”¨å“ªä¸ª Reward Functionã€‚
            # 'lighteval/MATH' å¯¹åº” verl/utils/reward_score/math_reward.pyï¼Œæ”¯æŒ latex æ ¼å¼çš„æ•°å­¦å…¬å¼åŒ¹é…
            "data_source": "lighteval/MATH", 
            
            # Prompt å¿…é¡»æ˜¯ Chat æ ¼å¼çš„ list
            "prompt": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": q}
            ],
            
            "ability": "math",
            
            # reward_model å­—æ®µåŒ…å«ç”¨äºè®¡ç®—å¥–åŠ±çš„çœŸå€¼ï¼ˆGround Truthï¼‰
            "reward_model": {
                "style": "rule", 
                "ground_truth": extract_solution(a) # æå–å‡ºçš„æ ‡å‡†ç­”æ¡ˆ
            },
            
            # é¢å¤–ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•æˆ–æ—¥å¿—
            "extra_info": {"split": "train", "index": i}
        })

    # è½¬æ¢ä¸º Pandas DataFrame
    df = pd.DataFrame(processed_data)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (95% è®­ç»ƒ, 5% éªŒè¯)
    train_size = int(len(df) * 0.95)
    train_df = df.iloc[:train_size]
    test_df = df.iloc[train_size:]
    
    # ä¿å­˜ä¸º Parquet æ–‡ä»¶
    train_path = os.path.join(output_dir, "train.parquet")
    test_path = os.path.join(output_dir, "test.parquet")
    
    train_df.to_parquet(train_path)
    test_df.to_parquet(test_path)
    
    print(f"æ•°æ®å·²ä¿å­˜: è®­ç»ƒé›† {len(train_df)} æ¡, éªŒè¯é›† {len(test_df)} æ¡")
    return train_path, test_path

def main():
    # 1. å‡†å¤‡æ•°æ®
    train_file, test_file = prepare_data()
    if not train_file:
        print("æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡º")
        return

    # 2. æŒ‡å®šåŸºç¡€æ¨¡å‹è·¯å¾„
    model_path = "meta-llama/Llama-3.1-8B-Instruct"  # Qwen/Qwen2.5-0.5B-Instruct  Qwen/Qwen3-4B-Instruct-2507  meta-llama/Llama-3.1-8B-Instruct
    
    # å…³é”®è¶…å‚æ•°é…ç½®
    n_gpus = 8
    train_batch_size = 128
    ppo_mini_batch_size = 64
    micro_batch_size_per_gpu = 8
    rollout_n = 16
    offload = False
    vllm_gpu_memory_utilization = 0.4 # ç»™ vLLM åˆ†é… 40% æ˜¾å­˜ï¼Œé¿å…åˆå§‹åŒ– OOM
    
    # 3. æ„é€ å¯åŠ¨å‘½ä»¤
    # æˆ‘ä»¬é€šè¿‡è°ƒç”¨ verl.trainer.main_ppo æ¨¡å—æ¥å¯åŠ¨è®­ç»ƒã€‚
    # æ‰€æœ‰çš„é…ç½®å‚æ•°éƒ½é€šè¿‡ Hydra æ ¼å¼ä¼ é€’ï¼ˆkey=valueï¼‰ã€‚
    cmd = [
        sys.executable, "-m", "verl.trainer.main_ppo",
        
        # =================================================================
        # ç®—æ³•æ ¸å¿ƒé…ç½® (GRPO)
        # =================================================================
        "algorithm.adv_estimator=grpo",       # æŒ‡å®šä½¿ç”¨ GRPO (Group Relative Policy Optimization) ç®—æ³•
        "algorithm.use_kl_in_reward=False",   # GRPO ç‰¹æ€§ï¼šä¸æŠŠ KL æ•£åº¦æƒ©ç½šç›´æ¥åŠ åœ¨ Reward é‡Œï¼Œè€Œæ˜¯ä½œä¸º Loss çš„ä¸€éƒ¨åˆ†
        "algorithm.kl_ctrl.kl_coef=0.001",    # KL æ•£åº¦ç³»æ•°ï¼Œé˜²æ­¢æ¨¡å‹åç¦»åŸºåº§æ¨¡å‹å¤ªè¿œ
        
        # =================================================================
        # æ•°æ®é…ç½®
        # =================================================================
        f"data.train_files={train_file}",     # è®­ç»ƒæ•°æ®è·¯å¾„
        f"data.val_files={test_file}",       # éªŒè¯æ•°æ®è·¯å¾„
        f"data.train_batch_size={train_batch_size}",         # å…¨å±€ Batch Sizeï¼šæ¯æ¬¡æ›´æ–°å‚æ•°æ—¶ä½¿ç”¨çš„æ•°æ®é‡ï¼ˆPromptæ•°é‡ï¼‰ã€‚è¶Šå¤§è¶Šç¨³ã€‚
        "data.max_prompt_length=4096",        # æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆé—®é¢˜é•¿åº¦ï¼‰ï¼Œè®¾å¤§ä¸€ç‚¹é˜²æ­¢æˆªæ–­
        "data.max_response_length=4096",      # æœ€å¤§è¾“å‡ºé•¿åº¦ï¼ˆæ€ç»´é“¾é•¿åº¦ï¼‰ï¼ŒGRPO éœ€è¦æ¨¡å‹è¾“å‡ºå¾ˆé•¿çš„æ€è€ƒè¿‡ç¨‹
        
        # =================================================================
        # æ¨¡å‹é…ç½®
        # =================================================================
        f"actor_rollout_ref.model.path={model_path}", # æ¨¡å‹è·¯å¾„
        "actor_rollout_ref.model.use_remove_padding=True", # å¼€å¯å» Padding ä¼˜åŒ–ï¼Œæå¤§æå‡è®­ç»ƒæ•ˆç‡
        
        # =================================================================
        # Rollout (æ¨ç†/ç”Ÿæˆ) é…ç½®
        # GRPO çš„æ ¸å¿ƒåœ¨äºï¼šå¯¹äºåŒä¸€ä¸ªé—®é¢˜ï¼Œç”Ÿæˆä¸€ç»„ï¼ˆGroupï¼‰ä¸åŒçš„å›ç­”
        # =================================================================
        f"actor_rollout_ref.rollout.n={rollout_n}",     # å…³é”®å‚æ•°ï¼šæ¯ä¸ª Prompt é‡‡æ · {rollout_n} ä¸ªå›ç­”ã€‚GRPO ä¼šå¯¹æ¯”è¿™ {rollout_n} ä¸ªå›ç­”æ¥è®¡ç®—ä¼˜åŠ¿ã€‚
        "actor_rollout_ref.rollout.name=vllm",# ä½¿ç”¨ vLLM ä½œä¸ºæ¨ç†å¼•æ“ï¼Œé€Ÿåº¦æå¿«
        f"actor_rollout_ref.rollout.gpu_memory_utilization={vllm_gpu_memory_utilization}", # é™åˆ¶ vLLM å ç”¨ 80% æ˜¾å­˜ï¼Œå‰©ä¸‹çš„ç•™ç»™è®­ç»ƒ
        "actor_rollout_ref.rollout.free_cache_engine=False",    # å…³é—­ vLLM æ˜¾å­˜å¸è½½ï¼ˆé¿å… AMD ç¯å¢ƒä¸‹çš„ sleep/wake_up æ­»é”ï¼‰ï¼Œæ˜¾å­˜å……è¶³æ—¶å»ºè®®å…³é—­

        # ã€æ–°å¢ã€‘å¼ºåˆ¶è®¾ç½®æ•°æ®å¹¶è¡Œåº¦ä¸º 8ï¼Œç¡®ä¿ 8 å¼ å¡éƒ½å‚ä¸æ¨ç†
        "actor_rollout_ref.rollout.data_parallel_size=8",

        "actor_rollout_ref.rollout.enforce_eager=True",         # AMD ROCm ç¯å¢ƒç‰¹å®šä¼˜åŒ–ï¼šå…³é—­ CUDA Graph é¿å…å…¼å®¹æ€§é—®é¢˜
        
        # æ¨ç†æ—¶çš„å¹¶è¡Œè®¾ç½®
        "actor_rollout_ref.rollout.tensor_model_parallel_size=1", # å•ä¸ªæ¨¡å‹ä¸åšå¼ é‡å¹¶è¡Œï¼ˆ0.5Bå¤ªå°äº†ï¼Œä¸éœ€è¦åˆ‡åˆ†ï¼‰
        # vLLM ç‰¹å®šä¼˜åŒ–å‚æ•°
        "actor_rollout_ref.rollout.enable_chunked_prefill=False", # å…³é—­ Chunked Prefill ä»¥é¿å…ä¸Šä¸‹æ–‡é•¿åº¦æ£€æŸ¥æŠ¥é”™
        "actor_rollout_ref.rollout.max_num_batched_tokens=16384", # å…è®¸ vLLM ä¸€æ¬¡å¤„ç†æ›´å¤šçš„ Token
        f"actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu={micro_batch_size_per_gpu}", # è®¡ç®—ç”Ÿæˆæ–‡æœ¬ LogProb æ—¶çš„ Batch Size
        
        # =================================================================
        # Actor (ç­–ç•¥æ¨¡å‹) è®­ç»ƒé…ç½®
        # è´Ÿè´£æ‰§è¡Œåå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
        # =================================================================
        f"actor_rollout_ref.actor.ppo_mini_batch_size={ppo_mini_batch_size}",        # PPO æ›´æ–°æ—¶çš„ Mini Batchã€‚å¿…é¡» <= train_batch_size ({train_batch_size})
        f"actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu={micro_batch_size_per_gpu}",# æ¯å¼ å¡æ¯æ¬¡å‰å‘ä¼ æ’­å¤„ç†çš„æ•°æ®é‡ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
        "actor_rollout_ref.actor.use_kl_loss=True",               # å¼€å¯ KL Loss è®¡ç®—
        "actor_rollout_ref.actor.kl_loss_coef=0.001",             # KL Loss çš„æƒé‡
        
        # FSDP (Fully Sharded Data Parallel) ä¼˜åŒ–é…ç½®
        # å› ä¸ºæ˜¾å­˜è¶³å¤Ÿå¤§ (256GB)ï¼Œæˆ‘ä»¬å…³é—­æ‰€æœ‰ Offloadï¼Œè®©å‚æ•°å¸¸é©»æ˜¾å­˜ï¼Œé€Ÿåº¦æœ€å¿«
        f"actor_rollout_ref.actor.fsdp_config.param_offload={offload}",
        f"actor_rollout_ref.actor.fsdp_config.optimizer_offload={offload}",
        
        # =================================================================
        # Reference (å‚è€ƒæ¨¡å‹) é…ç½®
        # ç”¨äºè®¡ç®— KL æ•£åº¦ï¼Œç¡®ä¿æ–°æ¨¡å‹ä¸â€œå¿˜æœ¬â€
        # =================================================================
        f"actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu={micro_batch_size_per_gpu}",
        f"actor_rollout_ref.ref.fsdp_config.param_offload={offload}",  # åŒæ ·å…³é—­ Offloadï¼Œè®¡ç®— KL é£å¿«
        
        # =================================================================
        # Trainer (è®­ç»ƒå™¨) å…¨å±€é…ç½®
        # =================================================================
        "trainer.total_epochs=3",             # è®­ç»ƒ 3 ä¸ª Epoch
        f"trainer.n_gpus_per_node={n_gpus}",          # ä½¿ç”¨ {n_gpus} å¼  GPU
        "trainer.nnodes=1",                   # å•æœºè®­ç»ƒ
        "trainer.project_name=verl_grpo_full_scale", # Wandb é¡¹ç›®å
        "trainer.experiment_name=qwen_05b_math_8gpu",# å®éªŒå
        "trainer.logger=['console','wandb']", # æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°å’Œwandb
        "trainer.test_freq=10",               # æ¯ 10 ä¸ª Step å°±åœ¨éªŒè¯é›†ä¸Šæµ‹ä¸€æ¬¡ï¼Œæ–¹ä¾¿è§‚å¯Ÿæ•ˆæœ
        "trainer.save_freq=-1",               # ä¸ä¿å­˜ Checkpoint (è®¾ä¸º -1)
    ]
    
    offload_status = "ON" if offload else "OFF"
    print("\n" + "="*50)
    print(f"ğŸš€ å¼€å§‹è¿è¡Œ {n_gpus}å¡ MI325 é«˜æ€§èƒ½ GRPO è®­ç»ƒ...")
    print(f"é…ç½®: å…¨é‡æ•°æ® | Batch={train_batch_size} | Rollout N={rollout_n} | Offload={offload_status}")
    print("="*50 + "\n")
    
    try:
        # å¤åˆ¶å½“å‰ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        # å¼€å¯å®Œæ•´é”™è¯¯æ ˆæ‰“å°ï¼Œæ–¹ä¾¿è°ƒè¯•
        env["HYDRA_FULL_ERROR"] = "1"
        # AMD ç¯å¢ƒå¸¸è§ä¼˜åŒ–ï¼šç¦ç”¨ NCCL P2Pï¼ˆæœ‰æ—¶ä¼šå¯¼è‡´æ­»é”ï¼‰ï¼Œæ”¹ç”¨å…±äº«å†…å­˜æˆ– Ring æ¨¡å¼
        env["NCCL_P2P_DISABLE"] = "1" 
        
        # æ‰§è¡Œè®­ç»ƒå‘½ä»¤
        subprocess.run(cmd, check=True, env=env)
    except subprocess.CalledProcessError as e:
        print(f"\nè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    except KeyboardInterrupt:
        print("\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ã€‚")

if __name__ == "__main__":
    main()
